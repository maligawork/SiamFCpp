{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43f945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import onnx\n",
    "from onnxsim import simplify\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import (quantize_dynamic,\n",
    "                                      QuantType,\n",
    "                                      QuantFormat,\n",
    "                                      quantize_static,\n",
    "                                      CalibrationDataReader)\n",
    "\n",
    "sys.path.append('../')\n",
    "from platforms.core.config import cfg\n",
    "from siamfcpp.pipeline.utils import (cxywh2xywh, get_crop,\n",
    "                                     get_subwindow_tracking,\n",
    "                                     xywh2cxywh, xyxy2cxywh)\n",
    "from siamfcpp.model.common_opr.common_block import xcorr_depthwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908b98f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bchw(im_patch):\n",
    "    im_patch = im_patch.transpose(2, 0, 1)\n",
    "    im_patch = im_patch[np.newaxis, :, :, :]\n",
    "    return im_patch.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdbad03",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93b027b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_init_fp32 = \"../models/onnx/backbone_init.onnx\"\n",
    "backbone_init_uint8 = \"../models/onnx_dynamic/backbone_init.onnx\"\n",
    "\n",
    "backbone_fp32 = \"../models/onnx/backbone.onnx\"\n",
    "backbone_uint8 = \"../models/onnx_dynamic/backbone.onnx\"\n",
    "\n",
    "head_fp32 = \"../models/onnx/head_5.onnx\"\n",
    "head_uint8 = \"../models/onnx_dynamic/head_5.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "670ce4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantized_model = quantize_dynamic(backbone_init_fp32, backbone_init_uint8, weight_type=QuantType.QUInt8)\n",
    "# quantized_model = quantize_dynamic(backbone_fp32, backbone_uint8, weight_type=QuantType.QUInt8)\n",
    "quantized_model = quantize_dynamic(head_fp32, head_uint8, weight_type=QuantType.QUInt8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360bbba4",
   "metadata": {},
   "source": [
    "### Static"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de29c014",
   "metadata": {},
   "source": [
    "*backbone_init*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad508fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32 = \"../models/onnx/backbone_init.onnx\"\n",
    "model_quant = \"../models/onnx_static/backbone_init.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1dd5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_image_folder = \"C:\\\\Users\\\\isd.illia.maliha\\\\work\\\\sorted_datasets\\\\background\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7802f7a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def _preprocess_images(images_folder: str, size_limit=0):\n",
    "    image_names = os.listdir(images_folder)\n",
    "    \n",
    "    if size_limit > 0 and len(image_names) >= size_limit:\n",
    "        batch_filenames = [image_names[i] for i in range(size_limit)]\n",
    "    else:\n",
    "        batch_filenames = image_names\n",
    "        \n",
    "    unconcatenated_batch_data = []\n",
    "\n",
    "\n",
    "    for image_name in tqdm_notebook(batch_filenames[:1]):\n",
    "        image_filepath = images_folder + \"/\" + image_name\n",
    "        image = cv2.imread(image_filepath).astype(np.float32)\n",
    "        \n",
    "        h,w,_ = image.shape\n",
    "        x = np.random.randint(0,0.7*w)\n",
    "        y = np.random.randint(0,0.7*h)\n",
    "        ww = np.random.randint(25,w-x)\n",
    "        hh = np.random.randint(25,h-y)\n",
    "        \n",
    "        box = xywh2cxywh([x,y,ww,hh])\n",
    "        target_pos, target_sz = box[:2], box[2:]\n",
    "\n",
    "        avg_chans = np.mean(image, axis=(0, 1))\n",
    "\n",
    "        im_z_crop, _ = get_crop(\n",
    "            image,\n",
    "            target_pos,\n",
    "            target_sz,\n",
    "            127,\n",
    "            avg_chans=avg_chans,\n",
    "            context_amount=0.5,\n",
    "            func_get_subwindow=get_subwindow_tracking,\n",
    "        )\n",
    "\n",
    "        im_z_crop = to_bchw(im_z_crop)\n",
    "\n",
    "        unconcatenated_batch_data.append(im_z_crop)\n",
    "        \n",
    "    batch_data = np.concatenate(np.expand_dims(unconcatenated_batch_data, axis=0), axis=0)\n",
    "    \n",
    "    return batch_data\n",
    "\n",
    "\n",
    "class DataReader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_image_folder: str, model_path: str):\n",
    "        self.enum_data = None\n",
    "\n",
    "        # Use inference session to get input shape.\n",
    "        session = onnxruntime.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "\n",
    "        # Convert image to input data\n",
    "        self.nchw_data_list = _preprocess_images(calibration_image_folder, size_limit=0)\n",
    "        \n",
    "        self.input_name = session.get_inputs()[0].name\n",
    "        self.datasize = len(self.nchw_data_list)\n",
    "    \n",
    "\n",
    "    def get_next(self):\n",
    "        if self.enum_data is None:\n",
    "            self.enum_data = iter(\n",
    "                [{self.input_name: self.nchw_data_list[idx]} for idx in range(self.datasize)]\n",
    "            )\n",
    "        return next(self.enum_data, None)\n",
    "\n",
    "    def rewind(self):\n",
    "        self.enum_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56577ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc731e199664dbb9c42eabcebb37bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reader = DataReader(calibration_image_folder, model_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c14911",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = quantize_static(model_fp32,\n",
    "                                  model_quant,\n",
    "                                  reader,\n",
    "                                  quant_format=QuantFormat.QDQ,\n",
    "                                  activation_type=QuantType.QInt8,\n",
    "                                  weight_type=QuantType.QInt8,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024bc0b2",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc795a",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1524f9",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea04a9b",
   "metadata": {},
   "source": [
    "*backbone*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2861b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32 = \"../models/onnx/backbone.onnx\"\n",
    "model_quant = \"../models/onnx_static/backbone.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d8939ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_image_folder = \"C:\\\\Users\\\\isd.illia.maliha\\\\work\\\\sorted_datasets\\\\background\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ffbb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_images(images_folder: str, size_limit=0):\n",
    "    image_names = os.listdir(images_folder)\n",
    "    \n",
    "    if size_limit > 0 and len(image_names) >= size_limit:\n",
    "        batch_filenames = [image_names[i] for i in range(size_limit)]\n",
    "    else:\n",
    "        batch_filenames = image_names\n",
    "        \n",
    "    unconcatenated_batch_data = []\n",
    "\n",
    "\n",
    "    for image_name in tqdm_notebook(batch_filenames[:1]):\n",
    "        image_filepath = images_folder + \"/\" + image_name\n",
    "        image = cv2.imread(image_filepath).astype(np.float32)\n",
    "        \n",
    "        h,w,_ = image.shape\n",
    "        x = np.random.randint(0,0.7*w)\n",
    "        y = np.random.randint(0,0.7*h)\n",
    "        ww = np.random.randint(25,w-x)\n",
    "        hh = np.random.randint(25,h-y)\n",
    "        \n",
    "        box = xywh2cxywh([x,y,ww,hh])\n",
    "        target_pos, target_sz = box[:2], box[2:]\n",
    "\n",
    "        avg_chans = np.mean(image, axis=(0, 1))\n",
    "\n",
    "        im_x_crop, scale_x = get_crop(\n",
    "            image,\n",
    "            target_pos,\n",
    "            target_sz,\n",
    "            127,\n",
    "            x_size=303,\n",
    "            avg_chans=avg_chans,\n",
    "            context_amount=cfg.context_amount,\n",
    "            func_get_subwindow=get_subwindow_tracking,\n",
    "        )\n",
    "\n",
    "        im_x_crop = to_bchw(im_x_crop)\n",
    "\n",
    "        unconcatenated_batch_data.append(im_x_crop)\n",
    "        \n",
    "    batch_data = np.concatenate(np.expand_dims(unconcatenated_batch_data, axis=0), axis=0)\n",
    "    \n",
    "    return batch_data\n",
    "\n",
    "\n",
    "class DataReader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_image_folder: str, model_path: str):\n",
    "        self.enum_data = None\n",
    "\n",
    "        # Use inference session to get input shape.\n",
    "        session = onnxruntime.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "\n",
    "        # Convert image to input data\n",
    "        self.nchw_data_list = _preprocess_images(calibration_image_folder, size_limit=0)\n",
    "        \n",
    "        self.input_name = session.get_inputs()[0].name\n",
    "        self.datasize = len(self.nchw_data_list)\n",
    "    \n",
    "\n",
    "    def get_next(self):\n",
    "        if self.enum_data is None:\n",
    "            self.enum_data = iter(\n",
    "                [{self.input_name: self.nchw_data_list[idx]} for idx in range(self.datasize)]\n",
    "            )\n",
    "        return next(self.enum_data, None)\n",
    "\n",
    "    def rewind(self):\n",
    "        self.enum_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3b9286b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78b8d58cd1c4a4a87e1f26845a8fadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reader = DataReader(calibration_image_folder, model_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1c0dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = quantize_static(model_fp32,\n",
    "                                  model_quant,\n",
    "                                  reader,\n",
    "                                  quant_format=QuantFormat.QDQ,\n",
    "                                  activation_type=QuantType.QInt8,\n",
    "                                  weight_type=QuantType.QInt8,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c973809b",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbd356b",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa279eb7",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e24a38",
   "metadata": {},
   "source": [
    "*head*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "231bb596",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_init_fp32 = \"../models/onnx/backbone_init.onnx\"\n",
    "backbone_fp32 = \"../models/onnx/backbone.onnx\"\n",
    "model_fp32 = \"../models/onnx/head_5.onnx\"\n",
    "model_opt = \"../models/onnx/head_opt.onnx\"\n",
    "\n",
    "model_quant = \"../models/onnx_static/head_5.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f99ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_image_folder = \"C:\\\\Users\\\\isd.illia.maliha\\\\work\\\\sorted_datasets\\\\background\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf9f05cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_images(backbone_init_path: str, backbone_path: str, images_folder: str, size_limit=0):\n",
    "    \n",
    "    bone_init = onnxruntime.InferenceSession(backbone_init_path, providers=['CPUExecutionProvider'])\n",
    "    bone = onnxruntime.InferenceSession(backbone_path, providers=['CPUExecutionProvider'])\n",
    "    \n",
    "    image_names = os.listdir(images_folder)\n",
    "    \n",
    "    if size_limit > 0 and len(image_names) >= size_limit:\n",
    "        batch_filenames = [image_names[i] for i in range(size_limit)]\n",
    "    else:\n",
    "        batch_filenames = image_names\n",
    "        \n",
    "    in1,in2 = [],[]\n",
    "    \n",
    "\n",
    "    for image_name in tqdm_notebook(batch_filenames[:1]):\n",
    "        image_filepath = images_folder + \"/\" + image_name\n",
    "        image = cv2.imread(image_filepath).astype(np.float32)\n",
    "        \n",
    "        h,w,_ = image.shape\n",
    "        x = np.random.randint(0,0.7*w)\n",
    "        y = np.random.randint(0,0.7*h)\n",
    "        ww = np.random.randint(25,w-x)\n",
    "        hh = np.random.randint(25,h-y)\n",
    "        \n",
    "        box = xywh2cxywh([x,y,ww,hh])\n",
    "        target_pos, target_sz = box[:2], box[2:]\n",
    "\n",
    "        avg_chans = np.mean(image, axis=(0, 1))\n",
    "\n",
    "        im_z_crop, _ = get_crop(\n",
    "            image,\n",
    "            target_pos,\n",
    "            target_sz,\n",
    "            127,\n",
    "            avg_chans=avg_chans,\n",
    "            context_amount=0.5,\n",
    "            func_get_subwindow=get_subwindow_tracking,\n",
    "        )\n",
    "        im_z_crop = to_bchw(im_z_crop)\n",
    "        \n",
    "        c_z_k, r_z_k = bone_init.run(None, {'input': im_z_crop})\n",
    "    \n",
    "    \n",
    "\n",
    "        im_x_crop, scale_x = get_crop(\n",
    "            image,\n",
    "            target_pos,\n",
    "            target_sz,\n",
    "            127,\n",
    "            x_size=303,\n",
    "            avg_chans=avg_chans,\n",
    "            context_amount=cfg.context_amount,\n",
    "            func_get_subwindow=get_subwindow_tracking,\n",
    "        )\n",
    "        im_x_crop = to_bchw(im_x_crop)\n",
    "    \n",
    "        c_x, r_x = bone.run(None, {'input': im_x_crop})\n",
    "        \n",
    "        c_out = xcorr_depthwise(torch.Tensor(c_x), torch.Tensor(c_z_k))\n",
    "        r_out = xcorr_depthwise(torch.Tensor(r_x), torch.Tensor(r_z_k))\n",
    "        \n",
    "        in1.append(c_out.numpy())\n",
    "        in2.append(r_out.numpy())\n",
    "\n",
    "        \n",
    "    batch_data1 = np.concatenate(np.expand_dims(in1, axis=0), axis=0)\n",
    "    batch_data2 = np.concatenate(np.expand_dims(in2, axis=0), axis=0)\n",
    "    \n",
    "    return [batch_data1, batch_data2]\n",
    "\n",
    "\n",
    "class DataReader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_image_folder: str, model_path: str, backbone_init_path: str, backbone_path: str):\n",
    "        self.enum_data = None\n",
    "\n",
    "        # Use inference session to get input shape.\n",
    "        session = onnxruntime.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "\n",
    "        # Convert image to input data\n",
    "        self.nchw_data_list = _preprocess_images(backbone_init_path, backbone_path, calibration_image_folder, size_limit=0)\n",
    "        \n",
    "        self.input_names = [session.get_inputs()[i].name for i in range(2)]\n",
    "        self.datasize = len(self.nchw_data_list[0])\n",
    "    \n",
    "\n",
    "    def get_next(self):\n",
    "        if self.enum_data is None:\n",
    "            self.enum_data = iter(\n",
    "                [{self.input_names[0]: self.nchw_data_list[0][idx],\n",
    "                  self.input_names[1]: self.nchw_data_list[1][idx]} for idx in range(self.datasize)]\n",
    "            )\n",
    "        return next(self.enum_data, None)\n",
    "\n",
    "    def rewind(self):\n",
    "        self.enum_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0679c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m onnxruntime.quantization.preprocess --input \"../models/onnx/head_5.onnx\" --output \"../models/onnx/head_opt.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8677f534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isd.illia.maliha\\AppData\\Local\\Temp\\ipykernel_20892\\1704575551.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for image_name in tqdm_notebook(batch_filenames[:1]):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6524746efef34f9dbd653ed9e2fc4bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reader = DataReader(calibration_image_folder, model_opt, backbone_init_fp32, backbone_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21e941a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = quantize_static(model_opt,\n",
    "                                  model_quant,\n",
    "                                  reader,\n",
    "                                  quant_format=QuantFormat.QDQ,\n",
    "                                  activation_type=QuantType.QInt8,\n",
    "                                  weight_type=QuantType.QInt8,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e88a08",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb03ca8",
   "metadata": {},
   "source": [
    "#### One input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8af9235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from siamfcpp.config.config_head import cfg as root_cfg\n",
    "from siamfcpp.config.config_head import specify_task \n",
    "from siamfcpp.engine.builder import build as tester_builder\n",
    "from siamfcpp.model import builder_head as model_builder\n",
    "from siamfcpp.pipeline import builder as pipeline_builder\n",
    "from siamfcpp.utils import complete_path_wt_root_in_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c67fba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = '../models/siamfcpp/test/vot/siamfcpp_alexnet.yaml'\n",
    "model_path = '../models/snapshots/siamfcpp_alexnet-got/epoch-17.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89091c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one = \"../models/onnx/head_5_one.onnx\"\n",
    "model_fp32 = \"../models/onnx/head_5.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2597db29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 10:32:43.200 | INFO     | siamfcpp.model.module_base:update_params:60 - Load pretrained SiamTrack parameters from: ../models/snapshots/siamfcpp_alexnet-got/epoch-17.pkl whose md5sum is 2a050730626f1b083baed91f9a5c4a52\n"
     ]
    }
   ],
   "source": [
    "exp_cfg_path = os.path.realpath(config)\n",
    "root_cfg.merge_from_file(exp_cfg_path)\n",
    "root_cfg.test.track.model.task_model.SiamTrack.pretrain_model_path = model_path\n",
    "\n",
    "root_cfg = root_cfg.test\n",
    "task, task_cfg = specify_task(root_cfg)\n",
    "task_cfg.freeze()\n",
    "\n",
    "model = model_builder.build(\"track\", task_cfg.model)\n",
    "torch_model = model.head\n",
    "torch_model.eval()\n",
    "\n",
    "onnx_model = onnxruntime.InferenceSession(model_fp32, providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5be38c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1 = torch.Tensor(np.random.uniform(size=(1,256,23,23)).astype(np.float32))\n",
    "inp2 = torch.Tensor(np.random.uniform(size=(1,256,23,23)).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a7f2ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-3.69548798e-06, grad_fn=<SumBackward0>)\n",
      "tensor(-1.51991844e-05, grad_fn=<SumBackward0>)\n",
      "tensor(-7.90357590e-05, grad_fn=<SumBackward0>)\n",
      "tensor(-9.24681081e-05, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch_out2 = model.head(inp1, inp2) \n",
    "onnx_out2 = onnx_model.run(None, {'input1': inp1.numpy(),\n",
    "                                  'input2': inp2.numpy()})\n",
    "\n",
    "\n",
    "for idx in range(4):\n",
    "    print(torch.sum(torch_out2[idx]-torch.Tensor(onnx_out2[idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22a03b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, main_model):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.main_model = main_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1,x2 = torch.split(x, 256, dim=1)\n",
    "        out = self.main_model(x1,x2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "833f32f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "tensor(0., grad_fn=<SumBackward0>)\n",
      "tensor(0., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inp = torch.cat([inp1.clone(), inp2.clone()], dim=1)\n",
    "\n",
    "model = MyModel(torch_model)\n",
    "model.eval()\n",
    "\n",
    "torch_out1 = model(inp)\n",
    "\n",
    "for idx in range(4):\n",
    "    print(torch.sum(torch_out2[idx]-torch_out1[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3d54f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input : Float(1, 512, 23, 23, strides=[270848, 529, 23, 1], requires_grad=0, device=cpu),\n",
      "      %main_model.cls_p5_conv1.conv.weight : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=1, device=cpu),\n",
      "      %main_model.cls_p5_conv1.conv.bias : Float(256, strides=[1], requires_grad=1, device=cpu),\n",
      "      %main_model.bbox_p5_conv1.conv.weight : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=1, device=cpu),\n",
      "      %main_model.bbox_p5_conv1.conv.bias : Float(256, strides=[1], requires_grad=1, device=cpu),\n",
      "      %main_model.cls_p5_conv2.conv.weight : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=1, device=cpu),\n",
      "      %main_model.cls_p5_conv2.conv.bias : Float(256, strides=[1], requires_grad=1, device=cpu),\n",
      "      %main_model.bbox_p5_conv2.conv.weight : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=1, device=cpu),\n",
      "      %main_model.bbox_p5_conv2.conv.bias : Float(256, strides=[1], requires_grad=1, device=cpu),\n",
      "      %87 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %88 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %90 : Float(256, 256, 3, 3, strides=[2304, 9, 3, 1], requires_grad=0, device=cpu),\n",
      "      %91 : Float(256, strides=[1], requires_grad=0, device=cpu),\n",
      "      %93 : Float(1, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %94 : Float(1, strides=[1], requires_grad=0, device=cpu),\n",
      "      %96 : Float(1, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %97 : Float(1, strides=[1], requires_grad=0, device=cpu),\n",
      "      %99 : Float(4, 256, 1, 1, strides=[256, 1, 1, 1], requires_grad=0, device=cpu),\n",
      "      %100 : Float(4, strides=[1], requires_grad=0, device=cpu),\n",
      "      %104 : Long(3, strides=[1], requires_grad=0, device=cpu),\n",
      "      %108 : Long(3, strides=[1], requires_grad=0, device=cpu)):\n",
      "  %input.1 : Float(1, 256, 23, 23, strides=[270848, 529, 23, 1], requires_grad=0, device=cpu), %input.3 : Float(1, 256, 23, 23, strides=[270848, 529, 23, 1], requires_grad=0, device=cpu) = onnx::Split[axis=1, split=[256, 256]](%input) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\_tensor.py:566:0\n",
      "  %input.7 : Float(1, 256, 21, 21, strides=[112896, 441, 21, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%input.1, %main_model.cls_p5_conv1.conv.weight, %main_model.cls_p5_conv1.conv.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443:0\n",
      "  %input.11 : Float(1, 256, 21, 21, strides=[112896, 441, 21, 1], requires_grad=1, device=cpu) = onnx::Relu(%input.7) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\functional.py:1442:0\n",
      "  %input.15 : Float(1, 256, 21, 21, strides=[112896, 441, 21, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%input.3, %main_model.bbox_p5_conv1.conv.weight, %main_model.bbox_p5_conv1.conv.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443:0\n",
      "  %input.19 : Float(1, 256, 21, 21, strides=[112896, 441, 21, 1], requires_grad=1, device=cpu) = onnx::Relu(%input.15) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\functional.py:1442:0\n",
      "  %input.23 : Float(1, 256, 19, 19, strides=[92416, 361, 19, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%input.11, %main_model.cls_p5_conv2.conv.weight, %main_model.cls_p5_conv2.conv.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443:0\n",
      "  %input.27 : Float(1, 256, 19, 19, strides=[92416, 361, 19, 1], requires_grad=1, device=cpu) = onnx::Relu(%input.23) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\functional.py:1442:0\n",
      "  %input.31 : Float(1, 256, 19, 19, strides=[92416, 361, 19, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%input.19, %main_model.bbox_p5_conv2.conv.weight, %main_model.bbox_p5_conv2.conv.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443:0\n",
      "  %input.35 : Float(1, 256, 19, 19, strides=[92416, 361, 19, 1], requires_grad=1, device=cpu) = onnx::Relu(%input.31) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\functional.py:1442:0\n",
      "  %input.43 : Float(1, 256, 17, 17, strides=[73984, 289, 17, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%input.27, %87, %88) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443:0\n",
      "  %fea : Float(1, 256, 17, 17, strides=[73984, 289, 17, 1], requires_grad=1, device=cpu) = onnx::Relu(%input.43) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\functional.py:1442:0\n",
      "  %input.55 : Float(1, 256, 17, 17, strides=[73984, 289, 17, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%input.35, %90, %91) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443:0\n",
      "  %input.59 : Float(1, 256, 17, 17, strides=[73984, 289, 17, 1], requires_grad=1, device=cpu) = onnx::Relu(%input.55) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\functional.py:1442:0\n",
      "  %92 : Float(1, 1, 17, 17, strides=[289, 289, 17, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%fea, %93, %94) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443:0\n",
      "  %64 : Float(1, 17, 17, 1, strides=[289, 17, 1, 289], requires_grad=1, device=cpu) = onnx::Transpose[perm=[0, 2, 3, 1]](%92) # C:\\Users\\isd.illia.maliha\\work\\SiamFCpp\\Notebooks\\..\\siamfcpp\\model\\task_head_new\\taskhead_impl\\track_head.py:98:0\n",
      "  %csl_score : Float(1, 289, 1, strides=[289, 1, 289], requires_grad=1, device=cpu) = onnx::Reshape(%64, %104) # C:\\Users\\isd.illia.maliha\\work\\SiamFCpp\\Notebooks\\..\\siamfcpp\\model\\task_head_new\\taskhead_impl\\track_head.py:99:0\n",
      "  %95 : Float(1, 1, 17, 17, strides=[289, 289, 17, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%fea, %96, %97) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443:0\n",
      "  %75 : Float(1, 17, 17, 1, strides=[289, 17, 1, 289], requires_grad=1, device=cpu) = onnx::Transpose[perm=[0, 2, 3, 1]](%95) # C:\\Users\\isd.illia.maliha\\work\\SiamFCpp\\Notebooks\\..\\siamfcpp\\model\\task_head_new\\taskhead_impl\\track_head.py:102:0\n",
      "  %ctr_score : Float(1, 289, 1, strides=[289, 1, 289], requires_grad=1, device=cpu) = onnx::Reshape(%75, %108) # C:\\Users\\isd.illia.maliha\\work\\SiamFCpp\\Notebooks\\..\\siamfcpp\\model\\task_head_new\\taskhead_impl\\track_head.py:103:0\n",
      "  %offsets : Float(1, 4, 17, 17, strides=[1156, 289, 17, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%input.59, %99, %100) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:443:0\n",
      "  return (%csl_score, %ctr_score, %offsets, %fea)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, \n",
    "                  inp,\n",
    "                  model_one, \n",
    "                  input_names=['input'],\n",
    "                  output_names=['csl_score', 'ctr_score', 'offsets', 'fea'],\n",
    "                  verbose=True, \n",
    "                  export_params=True, \n",
    "                  do_constant_folding=True,\n",
    "                  opset_version=11) \n",
    "\n",
    "simplified_model, check = simplify(model_one, skip_fuse_bn=False)\n",
    "onnx.save_model(simplified_model, model_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6021cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_one = onnxruntime.InferenceSession(model_one, providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4313afa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "onnx_out1 = onnx_model_one.run(None, {'input': inp.numpy()})\n",
    "\n",
    "\n",
    "for idx in range(4):\n",
    "    print(torch.sum(torch.Tensor(onnx_out1[idx])-torch.Tensor(onnx_out2[idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d185626",
   "metadata": {},
   "source": [
    "###### Quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ad0bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_init_fp32 = \"../models/onnx/backbone_init.onnx\"\n",
    "backbone_fp32 = \"../models/onnx/backbone.onnx\"\n",
    "model_fp32 = \"../models/onnx/head_5_one.onnx\"\n",
    "model_opt = \"../models/onnx/head_opt.onnx\"\n",
    "\n",
    "model_quant = \"../models/onnx_static/head_5_one.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16e303e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_image_folder = \"C:\\\\Users\\\\isd.illia.maliha\\\\work\\\\sorted_datasets\\\\background\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a10c29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_images(backbone_init_path: str, backbone_path: str, images_folder: str, size_limit=0):\n",
    "    \n",
    "    bone_init = onnxruntime.InferenceSession(backbone_init_path, providers=['CPUExecutionProvider'])\n",
    "    bone = onnxruntime.InferenceSession(backbone_path, providers=['CPUExecutionProvider'])\n",
    "    \n",
    "    image_names = os.listdir(images_folder)\n",
    "    \n",
    "    if size_limit > 0 and len(image_names) >= size_limit:\n",
    "        batch_filenames = [image_names[i] for i in range(size_limit)]\n",
    "    else:\n",
    "        batch_filenames = image_names\n",
    "        \n",
    "    inp = []\n",
    "    \n",
    "\n",
    "    for image_name in tqdm_notebook(batch_filenames[:1]):\n",
    "        image_filepath = images_folder + \"/\" + image_name\n",
    "        image = cv2.imread(image_filepath).astype(np.float32)\n",
    "        \n",
    "        h,w,_ = image.shape\n",
    "        x = np.random.randint(0,0.7*w)\n",
    "        y = np.random.randint(0,0.7*h)\n",
    "        ww = np.random.randint(25,w-x)\n",
    "        hh = np.random.randint(25,h-y)\n",
    "        \n",
    "        box = xywh2cxywh([x,y,ww,hh])\n",
    "        target_pos, target_sz = box[:2], box[2:]\n",
    "\n",
    "        avg_chans = np.mean(image, axis=(0, 1))\n",
    "\n",
    "        im_z_crop, _ = get_crop(\n",
    "            image,\n",
    "            target_pos,\n",
    "            target_sz,\n",
    "            127,\n",
    "            avg_chans=avg_chans,\n",
    "            context_amount=0.5,\n",
    "            func_get_subwindow=get_subwindow_tracking,\n",
    "        )\n",
    "        im_z_crop = to_bchw(im_z_crop)\n",
    "        \n",
    "        c_z_k, r_z_k = bone_init.run(None, {'input': im_z_crop})\n",
    "    \n",
    "    \n",
    "\n",
    "        im_x_crop, scale_x = get_crop(\n",
    "            image,\n",
    "            target_pos,\n",
    "            target_sz,\n",
    "            127,\n",
    "            x_size=303,\n",
    "            avg_chans=avg_chans,\n",
    "            context_amount=cfg.context_amount,\n",
    "            func_get_subwindow=get_subwindow_tracking,\n",
    "        )\n",
    "        im_x_crop = to_bchw(im_x_crop)\n",
    "    \n",
    "        c_x, r_x = bone.run(None, {'input': im_x_crop})\n",
    "        \n",
    "        c_out = xcorr_depthwise(torch.Tensor(c_x), torch.Tensor(c_z_k))\n",
    "        r_out = xcorr_depthwise(torch.Tensor(r_x), torch.Tensor(r_z_k))\n",
    "        \n",
    "        out = torch.cat([c_out, r_out], dim=1)\n",
    "        \n",
    "        inp.append(out.numpy())\n",
    "\n",
    "        \n",
    "    batch_data = np.concatenate(np.expand_dims(inp, axis=0), axis=0)\n",
    "    \n",
    "    return batch_data\n",
    "\n",
    "\n",
    "class DataReader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_image_folder: str, model_path: str, backbone_init_path: str, backbone_path: str):\n",
    "        self.enum_data = None\n",
    "\n",
    "        # Use inference session to get input shape.\n",
    "        session = onnxruntime.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "\n",
    "        # Convert image to input data\n",
    "        self.nchw_data_list = _preprocess_images(backbone_init_path, backbone_path, calibration_image_folder, size_limit=0)\n",
    "        \n",
    "        self.input_name = session.get_inputs()[0].name\n",
    "        self.datasize = len(self.nchw_data_list)\n",
    "    \n",
    "\n",
    "    def get_next(self):\n",
    "        if self.enum_data is None:\n",
    "            self.enum_data = iter(\n",
    "                [{self.input_name: self.nchw_data_list[idx]} for idx in range(self.datasize)]\n",
    "            )\n",
    "        return next(self.enum_data, None)\n",
    "\n",
    "    def rewind(self):\n",
    "        self.enum_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78b8004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m onnxruntime.quantization.preprocess --input \"../models/onnx/head_5_one.onnx\" --output \"../models/onnx/head_opt.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c3c2e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isd.illia.maliha\\AppData\\Local\\Temp\\ipykernel_20892\\3093862944.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for image_name in tqdm_notebook(batch_filenames[:1]):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c765d4bc5e4e3484048a1384dff960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reader = DataReader(calibration_image_folder, model_opt, backbone_init_fp32, backbone_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f14f0fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = quantize_static(model_opt,\n",
    "                                  model_quant,\n",
    "                                  reader,\n",
    "                                  quant_format=QuantFormat.QDQ,\n",
    "                                  activation_type=QuantType.QInt8,\n",
    "                                  weight_type=QuantType.QInt8,)\n",
    "\n",
    "simplified_model, check = simplify(model_quant, skip_fuse_bn=False)\n",
    "onnx.save_model(simplified_model, model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53fdf268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csl_score\n",
      "ctr_score\n",
      "offsets\n",
      "fea\n"
     ]
    }
   ],
   "source": [
    "session = onnxruntime.InferenceSession(model_quant, providers=['CPUExecutionProvider'])\n",
    "\n",
    "for x in session.get_outputs():\n",
    "    print(x.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2d9368",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
