{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43f945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import onnx\n",
    "from onnxsim import simplify\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import (quantize_dynamic,\n",
    "                                      QuantType,\n",
    "                                      QuantFormat,\n",
    "                                      quantize_static,\n",
    "                                      CalibrationDataReader)\n",
    "\n",
    "sys.path.append('../')\n",
    "from platforms.core.config import cfg\n",
    "from siamfcpp.pipeline.utils import (cxywh2xywh, get_crop,\n",
    "                                     get_subwindow_tracking,\n",
    "                                     xywh2cxywh, xyxy2cxywh)\n",
    "from siamfcpp.model.common_opr.common_block import xcorr_depthwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "908b98f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bchw(im_patch):\n",
    "    im_patch = im_patch.transpose(2, 0, 1)\n",
    "    im_patch = im_patch[np.newaxis, :, :, :]\n",
    "    return im_patch.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdbad03",
   "metadata": {},
   "source": [
    "### Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b027b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_init_fp32 = \"../models/onnx/backbone_init.onnx\"\n",
    "backbone_init_uint8 = \"../models/onnx_dynamic/backbone_init.onnx\"\n",
    "\n",
    "backbone_fp32 = \"../models/onnx/backbone.onnx\"\n",
    "backbone_uint8 = \"../models/onnx_dynamic/backbone.onnx\"\n",
    "\n",
    "head_fp32 = \"../models/onnx/head.onnx\"\n",
    "head_uint8 = \"../models/onnx_dynamic/head.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "670ce4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = quantize_dynamic(backbone_init_fp32, backbone_init_uint8, weight_type=QuantType.QUInt8)\n",
    "quantized_model = quantize_dynamic(backbone_fp32, backbone_uint8, weight_type=QuantType.QUInt8)\n",
    "quantized_model = quantize_dynamic(head_fp32, head_uint8, weight_type=QuantType.QUInt8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360bbba4",
   "metadata": {},
   "source": [
    "### Static"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de29c014",
   "metadata": {},
   "source": [
    "*backbone_init*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad508fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32 = \"../models/onnx/backbone_init.onnx\"\n",
    "model_quant = \"../models/onnx_static/backbone_init.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1dd5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_image_folder = \"C:\\\\Users\\\\isd.illia.maliha\\\\work\\\\sorted_datasets\\\\background\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7802f7a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def _preprocess_images(images_folder: str, size_limit=0):\n",
    "    image_names = os.listdir(images_folder)\n",
    "    \n",
    "    if size_limit > 0 and len(image_names) >= size_limit:\n",
    "        batch_filenames = [image_names[i] for i in range(size_limit)]\n",
    "    else:\n",
    "        batch_filenames = image_names\n",
    "        \n",
    "    unconcatenated_batch_data = []\n",
    "\n",
    "\n",
    "    for image_name in tqdm_notebook(batch_filenames[:1]):\n",
    "        image_filepath = images_folder + \"/\" + image_name\n",
    "        image = cv2.imread(image_filepath).astype(np.float32)\n",
    "        \n",
    "        h,w,_ = image.shape\n",
    "        x = np.random.randint(0,0.7*w)\n",
    "        y = np.random.randint(0,0.7*h)\n",
    "        ww = np.random.randint(25,w-x)\n",
    "        hh = np.random.randint(25,h-y)\n",
    "        \n",
    "        box = xywh2cxywh([x,y,ww,hh])\n",
    "        target_pos, target_sz = box[:2], box[2:]\n",
    "\n",
    "        avg_chans = np.mean(image, axis=(0, 1))\n",
    "\n",
    "        im_z_crop, _ = get_crop(\n",
    "            image,\n",
    "            target_pos,\n",
    "            target_sz,\n",
    "            127,\n",
    "            avg_chans=avg_chans,\n",
    "            context_amount=0.5,\n",
    "            func_get_subwindow=get_subwindow_tracking,\n",
    "        )\n",
    "\n",
    "        im_z_crop = to_bchw(im_z_crop)\n",
    "\n",
    "        unconcatenated_batch_data.append(im_z_crop)\n",
    "        \n",
    "    batch_data = np.concatenate(np.expand_dims(unconcatenated_batch_data, axis=0), axis=0)\n",
    "    \n",
    "    return batch_data\n",
    "\n",
    "\n",
    "class DataReader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_image_folder: str, model_path: str):\n",
    "        self.enum_data = None\n",
    "\n",
    "        # Use inference session to get input shape.\n",
    "        session = onnxruntime.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "\n",
    "        # Convert image to input data\n",
    "        self.nchw_data_list = _preprocess_images(calibration_image_folder, size_limit=0)\n",
    "        \n",
    "        self.input_name = session.get_inputs()[0].name\n",
    "        self.datasize = len(self.nchw_data_list)\n",
    "    \n",
    "\n",
    "    def get_next(self):\n",
    "        if self.enum_data is None:\n",
    "            self.enum_data = iter(\n",
    "                [{self.input_name: self.nchw_data_list[idx]} for idx in range(self.datasize)]\n",
    "            )\n",
    "        return next(self.enum_data, None)\n",
    "\n",
    "    def rewind(self):\n",
    "        self.enum_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56577ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc731e199664dbb9c42eabcebb37bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reader = DataReader(calibration_image_folder, model_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c14911",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = quantize_static(model_fp32,\n",
    "                                  model_quant,\n",
    "                                  reader,\n",
    "                                  quant_format=QuantFormat.QDQ,\n",
    "                                  activation_type=QuantType.QInt8,\n",
    "                                  weight_type=QuantType.QInt8,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024bc0b2",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc795a",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1524f9",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea04a9b",
   "metadata": {},
   "source": [
    "*backbone*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2861b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32 = \"../models/onnx/backbone.onnx\"\n",
    "model_quant = \"../models/onnx_static/backbone.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d8939ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_image_folder = \"C:\\\\Users\\\\isd.illia.maliha\\\\work\\\\sorted_datasets\\\\background\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67ffbb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_images(images_folder: str, size_limit=0):\n",
    "    image_names = os.listdir(images_folder)\n",
    "    \n",
    "    if size_limit > 0 and len(image_names) >= size_limit:\n",
    "        batch_filenames = [image_names[i] for i in range(size_limit)]\n",
    "    else:\n",
    "        batch_filenames = image_names\n",
    "        \n",
    "    unconcatenated_batch_data = []\n",
    "\n",
    "\n",
    "    for image_name in tqdm_notebook(batch_filenames[:1]):\n",
    "        image_filepath = images_folder + \"/\" + image_name\n",
    "        image = cv2.imread(image_filepath).astype(np.float32)\n",
    "        \n",
    "        h,w,_ = image.shape\n",
    "        x = np.random.randint(0,0.7*w)\n",
    "        y = np.random.randint(0,0.7*h)\n",
    "        ww = np.random.randint(25,w-x)\n",
    "        hh = np.random.randint(25,h-y)\n",
    "        \n",
    "        box = xywh2cxywh([x,y,ww,hh])\n",
    "        target_pos, target_sz = box[:2], box[2:]\n",
    "\n",
    "        avg_chans = np.mean(image, axis=(0, 1))\n",
    "\n",
    "        im_x_crop, scale_x = get_crop(\n",
    "            image,\n",
    "            target_pos,\n",
    "            target_sz,\n",
    "            127,\n",
    "            x_size=303,\n",
    "            avg_chans=avg_chans,\n",
    "            context_amount=cfg.context_amount,\n",
    "            func_get_subwindow=get_subwindow_tracking,\n",
    "        )\n",
    "\n",
    "        im_x_crop = to_bchw(im_x_crop)\n",
    "\n",
    "        unconcatenated_batch_data.append(im_x_crop)\n",
    "        \n",
    "    batch_data = np.concatenate(np.expand_dims(unconcatenated_batch_data, axis=0), axis=0)\n",
    "    \n",
    "    return batch_data\n",
    "\n",
    "\n",
    "class DataReader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_image_folder: str, model_path: str):\n",
    "        self.enum_data = None\n",
    "\n",
    "        # Use inference session to get input shape.\n",
    "        session = onnxruntime.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "\n",
    "        # Convert image to input data\n",
    "        self.nchw_data_list = _preprocess_images(calibration_image_folder, size_limit=0)\n",
    "        \n",
    "        self.input_name = session.get_inputs()[0].name\n",
    "        self.datasize = len(self.nchw_data_list)\n",
    "    \n",
    "\n",
    "    def get_next(self):\n",
    "        if self.enum_data is None:\n",
    "            self.enum_data = iter(\n",
    "                [{self.input_name: self.nchw_data_list[idx]} for idx in range(self.datasize)]\n",
    "            )\n",
    "        return next(self.enum_data, None)\n",
    "\n",
    "    def rewind(self):\n",
    "        self.enum_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3b9286b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b78b8d58cd1c4a4a87e1f26845a8fadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reader = DataReader(calibration_image_folder, model_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1c0dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = quantize_static(model_fp32,\n",
    "                                  model_quant,\n",
    "                                  reader,\n",
    "                                  quant_format=QuantFormat.QDQ,\n",
    "                                  activation_type=QuantType.QInt8,\n",
    "                                  weight_type=QuantType.QInt8,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c973809b",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbd356b",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa279eb7",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e24a38",
   "metadata": {},
   "source": [
    "*head*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "231bb596",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_init_fp32 = \"../models/onnx/backbone_init.onnx\"\n",
    "backbone_fp32 = \"../models/onnx/backbone.onnx\"\n",
    "model_fp32 = \"../models/onnx/head.onnx\"\n",
    "model_opt = \"../models/onnx/head_opt.onnx\"\n",
    "\n",
    "model_quant = \"../models/onnx_static/head.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f99ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_image_folder = \"C:\\\\Users\\\\isd.illia.maliha\\\\work\\\\sorted_datasets\\\\background\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf9f05cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_images(backbone_init_path: str, backbone_path: str, images_folder: str, size_limit=0):\n",
    "    \n",
    "    bone_init = onnxruntime.InferenceSession(backbone_init_path, providers=['CPUExecutionProvider'])\n",
    "    bone = onnxruntime.InferenceSession(backbone_path, providers=['CPUExecutionProvider'])\n",
    "    \n",
    "    image_names = os.listdir(images_folder)\n",
    "    \n",
    "    if size_limit > 0 and len(image_names) >= size_limit:\n",
    "        batch_filenames = [image_names[i] for i in range(size_limit)]\n",
    "    else:\n",
    "        batch_filenames = image_names\n",
    "        \n",
    "    in1,in2 = [],[]\n",
    "    \n",
    "\n",
    "    for image_name in tqdm_notebook(batch_filenames[:1]):\n",
    "        image_filepath = images_folder + \"/\" + image_name\n",
    "        image = cv2.imread(image_filepath).astype(np.float32)\n",
    "        \n",
    "        h,w,_ = image.shape\n",
    "        x = np.random.randint(0,0.7*w)\n",
    "        y = np.random.randint(0,0.7*h)\n",
    "        ww = np.random.randint(25,w-x)\n",
    "        hh = np.random.randint(25,h-y)\n",
    "        \n",
    "        box = xywh2cxywh([x,y,ww,hh])\n",
    "        target_pos, target_sz = box[:2], box[2:]\n",
    "\n",
    "        avg_chans = np.mean(image, axis=(0, 1))\n",
    "\n",
    "        im_z_crop, _ = get_crop(\n",
    "            image,\n",
    "            target_pos,\n",
    "            target_sz,\n",
    "            127,\n",
    "            avg_chans=avg_chans,\n",
    "            context_amount=0.5,\n",
    "            func_get_subwindow=get_subwindow_tracking,\n",
    "        )\n",
    "        im_z_crop = to_bchw(im_z_crop)\n",
    "        \n",
    "        c_z_k, r_z_k = bone_init.run(None, {'input': im_z_crop})\n",
    "    \n",
    "    \n",
    "\n",
    "        im_x_crop, scale_x = get_crop(\n",
    "            image,\n",
    "            target_pos,\n",
    "            target_sz,\n",
    "            127,\n",
    "            x_size=303,\n",
    "            avg_chans=avg_chans,\n",
    "            context_amount=cfg.context_amount,\n",
    "            func_get_subwindow=get_subwindow_tracking,\n",
    "        )\n",
    "        im_x_crop = to_bchw(im_x_crop)\n",
    "    \n",
    "        c_x, r_x = bone.run(None, {'input': im_x_crop})\n",
    "        \n",
    "        c_out = xcorr_depthwise(torch.Tensor(c_x), torch.Tensor(c_z_k))\n",
    "        r_out = xcorr_depthwise(torch.Tensor(r_x), torch.Tensor(r_z_k))\n",
    "        \n",
    "        in1.append(c_out.numpy())\n",
    "        in2.append(r_out.numpy())\n",
    "\n",
    "        \n",
    "    batch_data1 = np.concatenate(np.expand_dims(in1, axis=0), axis=0)\n",
    "    batch_data2 = np.concatenate(np.expand_dims(in2, axis=0), axis=0)\n",
    "    \n",
    "    return [batch_data1, batch_data2]\n",
    "\n",
    "\n",
    "class DataReader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_image_folder: str, model_path: str, backbone_init_path: str, backbone_path: str):\n",
    "        self.enum_data = None\n",
    "\n",
    "        # Use inference session to get input shape.\n",
    "        session = onnxruntime.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "\n",
    "        # Convert image to input data\n",
    "        self.nchw_data_list = _preprocess_images(backbone_init_path, backbone_path, calibration_image_folder, size_limit=0)\n",
    "        \n",
    "        self.input_names = [session.get_inputs()[i].name for i in range(2)]\n",
    "        self.datasize = len(self.nchw_data_list[0])\n",
    "    \n",
    "\n",
    "    def get_next(self):\n",
    "        if self.enum_data is None:\n",
    "            self.enum_data = iter(\n",
    "                [{self.input_names[0]: self.nchw_data_list[0][idx],\n",
    "                  self.input_names[1]: self.nchw_data_list[1][idx]} for idx in range(self.datasize)]\n",
    "            )\n",
    "        return next(self.enum_data, None)\n",
    "\n",
    "    def rewind(self):\n",
    "        self.enum_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0679c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m onnxruntime.quantization.preprocess --input \"../models/onnx/head.onnx\" --output \"../models/onnx/head_opt.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8677f534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isd.illia.maliha\\AppData\\Local\\Temp\\ipykernel_9092\\1704575551.py:16: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for image_name in tqdm_notebook(batch_filenames[:1]):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46bd8a57b7a3412db1e4eaaf2e9b0237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reader = DataReader(calibration_image_folder, model_opt, backbone_init_fp32, backbone_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21e941a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = quantize_static(model_opt,\n",
    "                                  model_quant,\n",
    "                                  reader,\n",
    "                                  quant_format=QuantFormat.QDQ,\n",
    "                                  activation_type=QuantType.QInt8,\n",
    "                                  weight_type=QuantType.QInt8,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e88a08",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb03ca8",
   "metadata": {},
   "source": [
    "#### One input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "89091c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_fp32 = \"../networks/onnx/backbone_bottleneck_pe.onnx\"\n",
    "model_fp32 = \"../networks/onnx/complete.onnx\"\n",
    "# model_one = \"../networks/onnx/complete_one.onnx\"\n",
    "model_one = \"../networks/onnx16/complete_one.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "cc18f60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########  Warning: Dilation is not valid in current code  ##########\n",
      "Building lite transformer encoder...\n",
      "head channel: 128\n"
     ]
    }
   ],
   "source": [
    "yaml_fname = '../experiments/stark_lightning_X_trt/baseline_rephead_4_lite_search5.yaml'\n",
    "checkpoint_name = \"../checkpoints/train/stark_lightning_X_trt/baseline_rephead_4_lite_search5/STARKLightningXtrt_ep0500.pth.tar\"\n",
    "\n",
    "update_config_from_file(yaml_fname)\n",
    "\n",
    "model = build_stark_lightning_x_trt(cfg, phase='test')\n",
    "model.load_state_dict(torch.load(checkpoint_name, map_location='cpu')['net'], strict=True)\n",
    "model = repvgg_model_convert(model)\n",
    "model.eval()\n",
    "\n",
    "backbone = model.backbone\n",
    "bottleneck = model.bottleneck\n",
    "position_embed = model.pos_emb_x\n",
    "transformer = model.transformer\n",
    "box_head = model.box_head\n",
    "box_head.coord_x = box_head.coord_x.cpu()\n",
    "box_head.coord_y = box_head.coord_y.cpu()\n",
    "torch_model = STARK(backbone, bottleneck, position_embed, transformer, box_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "32046cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model = onnxruntime.InferenceSession(model_fp32, providers=['CPUExecutionProvider'])\n",
    "torch_model.load_state_dict(torch.load('../networks/pytorch/complete.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b269b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "def get_data(bs=1, sz_x=256, hw_z=64, c=256):\n",
    "    img_x = torch.randn(bs, 3, sz_x, sz_x, requires_grad=True)\n",
    "    mask_x = torch.rand(bs, sz_x, sz_x, requires_grad=True) > 0.5\n",
    "    feat_vec_z = torch.randn(hw_z, bs, c, requires_grad=True)  # HWxBxC\n",
    "    mask_vec_z = torch.rand(bs, hw_z, requires_grad=True) > 0.5  # BxHW\n",
    "    pos_vec_z = torch.randn(hw_z, bs, c, requires_grad=True)  # HWxBxC\n",
    "    return img_x, mask_x, feat_vec_z, mask_vec_z, pos_vec_z\n",
    "\n",
    "\n",
    "bs = 1\n",
    "sz_x = cfg.TEST.SEARCH_SIZE\n",
    "hw_z = cfg.DATA.TEMPLATE.FEAT_SIZE ** 2\n",
    "c = cfg.MODEL.HIDDEN_DIM\n",
    "\n",
    "template, template_mask, feat_z, mask_z, pos_z = get_data(bs=bs, sz_x=sz_x, hw_z=hw_z, c=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3a7f2ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.04662704e-07, -5.96046448e-08,  1.63912773e-06,\n",
       "         1.19209290e-07]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt = torch_model(template, template_mask, feat_z, mask_z, pos_z) \n",
    "\n",
    "xo = onnx_model.run(None, {'img_x': to_numpy(template),\n",
    "                           'mask_x': to_numpy(template_mask), \n",
    "                           'feat_vec_z': to_numpy(feat_z), \n",
    "                           'mask_vec_z': to_numpy(mask_z),\n",
    "                           'pos_vec_z': to_numpy(pos_z)})[0]\n",
    "\n",
    "xt.detach().numpy().astype(float)-xo.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "22a03b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self, main_model):\n",
    "#         super(MyModel, self).__init__()\n",
    "#         self.main_model = main_model\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = x.reshape((-1,6,320,320))\n",
    "#         img_x, x2 = torch.split(x, 3, dim=1)\n",
    "        \n",
    "#         mask_x, fp_z, mask_vec_z = torch.split(x2, 1, dim=1)\n",
    "        \n",
    "#         mask_x = mask_x[:,0,...]\n",
    "        \n",
    "#         mask_vec_z = mask_vec_z[:,0,159,128:-128]\n",
    "#         mask_vec_z = mask_vec_z.type(torch.BoolTensor)\n",
    "# #         mask_vec_z = mask_vec_z > 0.5\n",
    "        \n",
    "#         fp_z = fp_z[...,96:-96,96:-96]\n",
    "#         feat_vec_z, pos_vec_z = torch.split(fp_z, 64, dim=2)\n",
    "        \n",
    "#         feat_vec_z = feat_vec_z[:,0,...]\n",
    "#         feat_vec_z = feat_vec_z.permute((1,0,2))\n",
    "\n",
    "#         pos_vec_z  = pos_vec_z[:,0,...]\n",
    "#         pos_vec_z  = pos_vec_z.permute((1,0,2))\n",
    "\n",
    "#         out = self.main_model(img_x, mask_x, feat_vec_z, mask_vec_z, pos_vec_z)\n",
    "\n",
    "#         return out\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, main_model):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.main_model = main_model\n",
    "        \n",
    "    def forward(self, x1,x2,x3,x4,x5):\n",
    "        \n",
    "        x2 = x2.type(torch.BoolTensor)\n",
    "        x4 = x4.type(torch.BoolTensor)\n",
    "        out = self.main_model(x1,x2,x3,x4,x5)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c3565539",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_x = template.clone()\n",
    "mask_x = template_mask.clone()\n",
    "feat_vec_z = feat_z.clone()\n",
    "mask_vec_z = mask_z.clone()\n",
    "pos_vec_z = pos_z.clone()\n",
    "\n",
    "# mask_x = mask_x[:,None]\n",
    "\n",
    "# feat_vec_z = feat_vec_z.permute((1,0,2))[:,None]\n",
    "# pos_vec_z  = pos_vec_z.permute((1,0,2))[:,None]\n",
    "# fp_z = torch.cat([feat_vec_z, pos_vec_z], dim=2)\n",
    "# fp_z = F.pad(fp_z,(96,96,96,96))\n",
    "\n",
    "# mask_vec_z = mask_vec_z[:, None, None]\n",
    "# mask_vec_z = F.pad(mask_vec_z,(128,128,159,160))\n",
    "\n",
    "# inp = torch.cat([img_x, mask_x, fp_z, mask_vec_z], dim=1)\n",
    "# inp = inp.reshape((1,3,320,640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "833f32f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0., grad_fn=<SumBackward0>),\n",
       " tensor(-2.5034e-06, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModel(torch_model)\n",
    "# x_new = model(inp)\n",
    "x_new = model(img_x, mask_x.type(torch.FloatTensor), feat_vec_z, mask_vec_z.type(torch.FloatTensor), pos_vec_z)\n",
    "\n",
    "torch.sum(xt-x_new), torch.sum(torch.Tensor(xo)-x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3efe5b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4432, 0.4292, 0.2356, 0.1382]], grad_fn=<StackBackward>),\n",
       " tensor([[0.4432, 0.4292, 0.2356, 0.1382]]),\n",
       " tensor([[0.4432, 0.4292, 0.2356, 0.1382]], grad_fn=<StackBackward>))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt,torch.Tensor(xo),x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a3d54f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%img_x : Float(1:307200, 3:102400, 320:320, 320:1, requires_grad=1, device=cpu),\n",
      "      %mask_x : Float(1:102400, 320:320, 320:1, requires_grad=0, device=cpu),\n",
      "      %feat_vec_z : Float(64:128, 1:128, 128:1, requires_grad=1, device=cpu),\n",
      "      %mask_vec_z : Float(1:64, 64:1, requires_grad=0, device=cpu),\n",
      "      %pos_vec_z : Float(64:128, 1:128, 128:1, requires_grad=1, device=cpu),\n",
      "      %main_model.backbone.body.stage0.rbr_reparam.weight : Float(48:27, 3:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage0.rbr_reparam.bias : Float(48:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage1.0.rbr_reparam.weight : Float(48:432, 48:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage1.0.rbr_reparam.bias : Float(48:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage1.1.rbr_reparam.weight : Float(48:432, 48:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage1.1.rbr_reparam.bias : Float(48:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage2.0.rbr_reparam.weight : Float(96:432, 48:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage2.0.rbr_reparam.bias : Float(96:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage2.1.rbr_reparam.weight : Float(96:864, 96:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage2.1.rbr_reparam.bias : Float(96:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage2.2.rbr_reparam.weight : Float(96:864, 96:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage2.2.rbr_reparam.bias : Float(96:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage2.3.rbr_reparam.weight : Float(96:864, 96:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage2.3.rbr_reparam.bias : Float(96:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage3.0.rbr_reparam.weight : Float(192:864, 96:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage3.0.rbr_reparam.bias : Float(192:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage3.1.rbr_reparam.weight : Float(192:1728, 192:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage3.1.rbr_reparam.bias : Float(192:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage3.2.rbr_reparam.weight : Float(192:1728, 192:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage3.2.rbr_reparam.bias : Float(192:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage3.3.rbr_reparam.weight : Float(192:1728, 192:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
      "      %main_model.backbone.body.stage3.3.rbr_reparam.bias : Float(192:1, requires_grad=0, device=cpu),\n",
      "      %main_model.bottleneck.weight : Float(128:192, 192:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
      "      %main_model.bottleneck.bias : Float(128:1, requires_grad=1, device=cpu),\n",
      "      %main_model.position_embed.row_embed.weight : Float(20:64, 64:1, requires_grad=1, device=cpu),\n",
      "      %main_model.position_embed.col_embed.weight : Float(20:64, 64:1, requires_grad=1, device=cpu),\n",
      "      %main_model.transformer.encoder.self_attn.in_proj_weight : Float(384:128, 128:1, requires_grad=1, device=cpu),\n",
      "      %main_model.transformer.encoder.self_attn.in_proj_bias : Float(384:1, requires_grad=1, device=cpu),\n",
      "      %main_model.transformer.encoder.self_attn.out_proj.bias : Float(128:1, requires_grad=1, device=cpu),\n",
      "      %main_model.transformer.encoder.linear1.bias : Float(1024:1, requires_grad=1, device=cpu),\n",
      "      %main_model.transformer.encoder.linear2.bias : Float(128:1, requires_grad=1, device=cpu),\n",
      "      %main_model.transformer.encoder.norm1.weight : Float(128:1, requires_grad=1, device=cpu),\n",
      "      %main_model.transformer.encoder.norm1.bias : Float(128:1, requires_grad=1, device=cpu),\n",
      "      %main_model.transformer.encoder.norm2.weight : Float(128:1, requires_grad=1, device=cpu),\n",
      "      %main_model.transformer.encoder.norm2.bias : Float(128:1, requires_grad=1, device=cpu),\n",
      "      %main_model.box_head.conv_tower.0.rbr_reparam.weight : Float(128:1152, 128:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
      "      %main_model.box_head.conv_tower.0.rbr_reparam.bias : Float(128:1, requires_grad=0, device=cpu),\n",
      "      %main_model.box_head.conv_tower.1.rbr_reparam.weight : Float(128:1152, 128:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
      "      %main_model.box_head.conv_tower.1.rbr_reparam.bias : Float(128:1, requires_grad=0, device=cpu),\n",
      "      %main_model.box_head.conv_tower.2.weight : Float(2:1152, 128:9, 3:3, 3:1, requires_grad=1, device=cpu),\n",
      "      %main_model.box_head.conv_tower.2.bias : Float(2:1, requires_grad=1, device=cpu),\n",
      "      %388 : Long(1:1, requires_grad=0, device=cpu),\n",
      "      %389 : Long(1:1, requires_grad=0, device=cpu),\n",
      "      %390 : Long(1:1, requires_grad=0, device=cpu),\n",
      "      %391 : Long(1:1, requires_grad=0, device=cpu),\n",
      "      %392 : Long(1:1, requires_grad=0, device=cpu),\n",
      "      %393 : Long(1:1, requires_grad=0, device=cpu),\n",
      "      %394 : Long(1:1, requires_grad=0, device=cpu),\n",
      "      %395 : Long(1:1, requires_grad=0, device=cpu),\n",
      "      %396 : Long(1:1, requires_grad=0, device=cpu),\n",
      "      %397 : Long(1:1, requires_grad=0, device=cpu),\n",
      "      %398 : Long(1:1, requires_grad=0, device=cpu),\n",
      "      %399 : Long(1:1, requires_grad=0, device=cpu),\n",
      "      %400 : Long(1:1, requires_grad=0, device=cpu),\n",
      "      %401 : Long(1:1, requires_grad=0, device=cpu),\n",
      "      %402 : Long(1:1, requires_grad=0, device=cpu),\n",
      "      %403 : Long(1:1, requires_grad=0, device=cpu),\n",
      "      %404 : Float(128:1, 128:128, requires_grad=0, device=cpu),\n",
      "      %405 : Float(requires_grad=0, device=cpu),\n",
      "      %406 : Float(128:1, 1024:128, requires_grad=0, device=cpu),\n",
      "      %407 : Float(1024:1, 128:1024, requires_grad=0, device=cpu),\n",
      "      %408 : Float(requires_grad=0, device=cpu),\n",
      "      %409 : Long(1:1, requires_grad=0, device=cpu),\n",
      "      %410 : Long(1:1, requires_grad=0, device=cpu),\n",
      "      %411 : Float(requires_grad=0, device=cpu),\n",
      "      %412 : Float(requires_grad=0, device=cpu),\n",
      "      %413 : Float(requires_grad=0, device=cpu)):\n",
      "  %49 : Bool(1:102400, 320:320, 320:1, requires_grad=0, device=cpu) = onnx::Cast[to=9](%mask_x) # <ipython-input-169-a74ced15959a>:38:0\n",
      "  %50 : Bool(1:64, 64:1, requires_grad=0, device=cpu) = onnx::Cast[to=9](%mask_vec_z) # <ipython-input-169-a74ced15959a>:39:0\n",
      "  %51 : Float(1:1228800, 48:25600, 160:160, 160:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%img_x, %main_model.backbone.body.stage0.rbr_reparam.weight, %main_model.backbone.body.stage0.rbr_reparam.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\conv.py:420:0\n",
      "  %52 : Float(1:1228800, 48:25600, 160:160, 160:1, requires_grad=1, device=cpu) = onnx::Relu(%51) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1136:0\n",
      "  %53 : Float(1:307200, 48:6400, 80:80, 80:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%52, %main_model.backbone.body.stage1.0.rbr_reparam.weight, %main_model.backbone.body.stage1.0.rbr_reparam.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\conv.py:420:0\n",
      "  %54 : Float(1:307200, 48:6400, 80:80, 80:1, requires_grad=1, device=cpu) = onnx::Relu(%53) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1136:0\n",
      "  %55 : Float(1:307200, 48:6400, 80:80, 80:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%54, %main_model.backbone.body.stage1.1.rbr_reparam.weight, %main_model.backbone.body.stage1.1.rbr_reparam.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\conv.py:420:0\n",
      "  %56 : Float(1:307200, 48:6400, 80:80, 80:1, requires_grad=1, device=cpu) = onnx::Relu(%55) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1136:0\n",
      "  %57 : Float(1:153600, 96:1600, 40:40, 40:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%56, %main_model.backbone.body.stage2.0.rbr_reparam.weight, %main_model.backbone.body.stage2.0.rbr_reparam.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\conv.py:420:0\n",
      "  %58 : Float(1:153600, 96:1600, 40:40, 40:1, requires_grad=1, device=cpu) = onnx::Relu(%57) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1136:0\n",
      "  %59 : Float(1:153600, 96:1600, 40:40, 40:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%58, %main_model.backbone.body.stage2.1.rbr_reparam.weight, %main_model.backbone.body.stage2.1.rbr_reparam.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\conv.py:420:0\n",
      "  %60 : Float(1:153600, 96:1600, 40:40, 40:1, requires_grad=1, device=cpu) = onnx::Relu(%59) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1136:0\n",
      "  %61 : Float(1:153600, 96:1600, 40:40, 40:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%60, %main_model.backbone.body.stage2.2.rbr_reparam.weight, %main_model.backbone.body.stage2.2.rbr_reparam.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\conv.py:420:0\n",
      "  %62 : Float(1:153600, 96:1600, 40:40, 40:1, requires_grad=1, device=cpu) = onnx::Relu(%61) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1136:0\n",
      "  %63 : Float(1:153600, 96:1600, 40:40, 40:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%62, %main_model.backbone.body.stage2.3.rbr_reparam.weight, %main_model.backbone.body.stage2.3.rbr_reparam.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\conv.py:420:0\n",
      "  %64 : Float(1:153600, 96:1600, 40:40, 40:1, requires_grad=1, device=cpu) = onnx::Relu(%63) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1136:0\n",
      "  %65 : Float(1:76800, 192:400, 20:20, 20:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[2, 2]](%64, %main_model.backbone.body.stage3.0.rbr_reparam.weight, %main_model.backbone.body.stage3.0.rbr_reparam.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\conv.py:420:0\n",
      "  %66 : Float(1:76800, 192:400, 20:20, 20:1, requires_grad=1, device=cpu) = onnx::Relu(%65) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1136:0\n",
      "  %67 : Float(1:76800, 192:400, 20:20, 20:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%66, %main_model.backbone.body.stage3.1.rbr_reparam.weight, %main_model.backbone.body.stage3.1.rbr_reparam.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\conv.py:420:0\n",
      "  %68 : Float(1:76800, 192:400, 20:20, 20:1, requires_grad=1, device=cpu) = onnx::Relu(%67) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1136:0\n",
      "  %69 : Float(1:76800, 192:400, 20:20, 20:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%68, %main_model.backbone.body.stage3.2.rbr_reparam.weight, %main_model.backbone.body.stage3.2.rbr_reparam.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\conv.py:420:0\n",
      "  %70 : Float(1:76800, 192:400, 20:20, 20:1, requires_grad=1, device=cpu) = onnx::Relu(%69) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1136:0\n",
      "  %71 : Float(1:76800, 192:400, 20:20, 20:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%70, %main_model.backbone.body.stage3.3.rbr_reparam.weight, %main_model.backbone.body.stage3.3.rbr_reparam.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\conv.py:420:0\n",
      "  %72 : Float(1:76800, 192:400, 20:20, 20:1, requires_grad=1, device=cpu) = onnx::Relu(%71) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1136:0\n",
      "  %73 : Float(1:51200, 128:400, 20:20, 20:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%72, %main_model.bottleneck.weight, %main_model.bottleneck.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\conv.py:420:0\n",
      "  %74 : Bool(1:102400, 1:102400, 320:320, 320:1, requires_grad=0, device=cpu) = onnx::Unsqueeze[axes=[0]](%49) # ..\\tracking\\ORT_lightning_X_trt_complete.py:53:0\n",
      "  %75 : Float(1:102400, 1:102400, 320:320, 320:1, requires_grad=0, device=cpu) = onnx::Cast[to=1](%74) # ..\\tracking\\ORT_lightning_X_trt_complete.py:53:0\n",
      "  %76 : Tensor = onnx::Shape(%73)\n",
      "  %77 : Tensor = onnx::Constant[value={2}]()\n",
      "  %78 : Long(device=cpu) = onnx::Gather[axis=0](%76, %77) # ..\\tracking\\ORT_lightning_X_trt_complete.py:53:0\n",
      "  %79 : Tensor = onnx::Shape(%73)\n",
      "  %80 : Tensor = onnx::Constant[value={3}]()\n",
      "  %81 : Long(device=cpu) = onnx::Gather[axis=0](%79, %80) # ..\\tracking\\ORT_lightning_X_trt_complete.py:53:0\n",
      "  %82 : Tensor = onnx::Unsqueeze[axes=[0]](%78)\n",
      "  %83 : Tensor = onnx::Unsqueeze[axes=[0]](%81)\n",
      "  %84 : Tensor = onnx::Concat[axis=0](%82, %83)\n",
      "  %85 : Tensor = onnx::Constant[value=[ CPUFloatType{0} ]]()\n",
      "  %86 : Tensor = onnx::Shape(%75)\n",
      "  %87 : Tensor = onnx::Constant[value={0}]()\n",
      "  %88 : Tensor = onnx::Constant[value={0}]()\n",
      "  %89 : Tensor = onnx::Constant[value={2}]()\n",
      "  %90 : Tensor = onnx::Slice(%86, %88, %89, %87)\n",
      "  %91 : Tensor = onnx::Cast[to=7](%84)\n",
      "  %92 : Tensor = onnx::Concat[axis=0](%90, %91)\n",
      "  %93 : Tensor = onnx::Constant[value=[ CPUFloatType{0} ]]()\n",
      "  %94 : Float(1:400, 1:400, 20:20, 20:1, requires_grad=0, device=cpu) = onnx::Resize[coordinate_transformation_mode=\"asymmetric\", cubic_coeff_a=-0.75, mode=\"nearest\", nearest_mode=\"floor\"](%75, %85, %93, %92) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:3132:0\n",
      "  %95 : Bool(1:400, 1:400, 20:20, 20:1, requires_grad=0, device=cpu) = onnx::Cast[to=9](%94) # ..\\tracking\\ORT_lightning_X_trt_complete.py:53:0\n",
      "  %96 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n",
      "  %97 : Bool(1:400, 20:20, 20:1, requires_grad=0, device=cpu) = onnx::Gather[axis=0](%95, %96) # ..\\tracking\\ORT_lightning_X_trt_complete.py:53:0\n",
      "  %98 : Long(20:1, requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>]()\n",
      "  %99 : Long(20:1, requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>]()\n",
      "  %100 : Float(20:64, 64:1, requires_grad=1, device=cpu) = onnx::Gather(%main_model.position_embed.col_embed.weight, %98) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1852:0\n",
      "  %101 : Float(20:64, 64:1, requires_grad=1, device=cpu) = onnx::Gather(%main_model.position_embed.row_embed.weight, %99) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1852:0\n",
      "  %102 : Float(1:1280, 20:64, 64:1, requires_grad=1, device=cpu) = onnx::Unsqueeze[axes=[0]](%100) # ..\\lib\\models\\stark\\position_encoding.py:131:0\n",
      "  %103 : int[] = onnx::Constant[value= 20   1   1 [ CPULongType{3} ]]()\n",
      "  %105 : Tensor = onnx::ConstantOfShape[value={1}](%388)\n",
      "  %106 : Tensor = onnx::Expand(%102, %105)\n",
      "  %107 : Float(20:1280, 20:64, 64:1, requires_grad=1, device=cpu) = onnx::Tile(%106, %103) # ..\\lib\\models\\stark\\position_encoding.py:131:0\n",
      "  %108 : Float(20:64, 1:64, 64:1, requires_grad=1, device=cpu) = onnx::Unsqueeze[axes=[1]](%101) # ..\\lib\\models\\stark\\position_encoding.py:132:0\n",
      "  %109 : int[] = onnx::Constant[value=  1  20   1 [ CPULongType{3} ]]()\n",
      "  %111 : Tensor = onnx::ConstantOfShape[value={1}](%389)\n",
      "  %112 : Tensor = onnx::Expand(%108, %111)\n",
      "  %113 : Float(20:1280, 20:64, 64:1, requires_grad=1, device=cpu) = onnx::Tile(%112, %109) # ..\\lib\\models\\stark\\position_encoding.py:132:0\n",
      "  %114 : Float(20:2560, 20:128, 128:1, requires_grad=1, device=cpu) = onnx::Concat[axis=-1](%107, %113) # ..\\lib\\models\\stark\\position_encoding.py:133:0\n",
      "  %115 : Float(128:1, 20:2560, 20:128, requires_grad=1, device=cpu) = onnx::Transpose[perm=[2, 0, 1]](%114) # ..\\lib\\models\\stark\\position_encoding.py:133:0\n",
      "  %116 : Float(1:128, 128:1, 20:2560, 20:128, requires_grad=1, device=cpu) = onnx::Unsqueeze[axes=[0]](%115) # ..\\lib\\models\\stark\\position_encoding.py:133:0\n",
      "  %117 : int[] = onnx::Constant[value= 1  1  1  1 [ CPULongType{4} ]]()\n",
      "  %119 : Tensor = onnx::ConstantOfShape[value={1}](%390)\n",
      "  %120 : Tensor = onnx::Expand(%116, %119)\n",
      "  %121 : Float(1:51200, 128:400, 20:20, 20:1, requires_grad=1, device=cpu) = onnx::Tile(%120, %117) # ..\\lib\\models\\stark\\position_encoding.py:133:0\n",
      "  %122 : Tensor = onnx::Shape(%73)\n",
      "  %123 : Tensor = onnx::Constant[value={0}]()\n",
      "  %124 : Tensor = onnx::Constant[value={0}]()\n",
      "  %125 : Tensor = onnx::Constant[value={2}]()\n",
      "  %126 : Tensor = onnx::Slice(%122, %124, %125, %123)\n",
      "  %127 : Tensor = onnx::Constant[value={-1}]()\n",
      "  %128 : Tensor = onnx::Concat[axis=0](%126, %127)\n",
      "  %129 : Float(1:51200, 128:400, 400:1, requires_grad=1, device=cpu) = onnx::Reshape(%73, %128) # ..\\tracking\\ORT_lightning_X_trt_complete.py:56:0\n",
      "  %130 : Float(400:1, 1:51200, 128:400, requires_grad=1, device=cpu) = onnx::Transpose[perm=[2, 0, 1]](%129) # ..\\tracking\\ORT_lightning_X_trt_complete.py:56:0\n",
      "  %131 : Tensor = onnx::Shape(%121)\n",
      "  %132 : Tensor = onnx::Constant[value={0}]()\n",
      "  %133 : Tensor = onnx::Constant[value={0}]()\n",
      "  %134 : Tensor = onnx::Constant[value={2}]()\n",
      "  %135 : Tensor = onnx::Slice(%131, %133, %134, %132)\n",
      "  %136 : Tensor = onnx::Constant[value={-1}]()\n",
      "  %137 : Tensor = onnx::Concat[axis=0](%135, %136)\n",
      "  %138 : Float(1:51200, 128:400, 400:1, requires_grad=1, device=cpu) = onnx::Reshape(%121, %137) # ..\\tracking\\ORT_lightning_X_trt_complete.py:57:0\n",
      "  %139 : Float(400:1, 1:51200, 128:400, requires_grad=1, device=cpu) = onnx::Transpose[perm=[2, 0, 1]](%138) # ..\\tracking\\ORT_lightning_X_trt_complete.py:57:0\n",
      "  %140 : Bool(1:400, 400:1, requires_grad=0, device=cpu) = onnx::Flatten[axis=1](%97) # ..\\tracking\\ORT_lightning_X_trt_complete.py:58:0\n",
      "  %141 : Float(464:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::Concat[axis=0](%feat_vec_z, %130) # ..\\tracking\\ORT_lightning_X_trt_complete.py:60:0\n",
      "  %142 : Bool(1:464, 464:1, requires_grad=0, device=cpu) = onnx::Concat[axis=1](%50, %140) # ..\\tracking\\ORT_lightning_X_trt_complete.py:61:0\n",
      "  %143 : Float(464:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::Concat[axis=0](%pos_vec_z, %139) # ..\\tracking\\ORT_lightning_X_trt_complete.py:62:0\n",
      "  %144 : Float(400:1, 1:51200, 128:400, requires_grad=1, device=cpu) = onnx::Add(%130, %139) # ..\\tracking\\ORT_lightning_X_trt_complete.py:64:0\n",
      "  %145 : Float(464:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::Add(%141, %143) # ..\\tracking\\ORT_lightning_X_trt_complete.py:65:0\n",
      "  %146 : Tensor = onnx::Shape(%144)\n",
      "  %147 : Tensor = onnx::Constant[value={0}]()\n",
      "  %148 : Long(device=cpu) = onnx::Gather[axis=0](%146, %147) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4134:0\n",
      "  %149 : Tensor = onnx::Shape(%144)\n",
      "  %150 : Tensor = onnx::Constant[value={1}]()\n",
      "  %151 : Long(device=cpu) = onnx::Gather[axis=0](%149, %150) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4134:0\n",
      "  %152 : Tensor = onnx::Shape(%144)\n",
      "  %153 : Tensor = onnx::Constant[value={2}]()\n",
      "  %154 : Long(device=cpu) = onnx::Gather[axis=0](%152, %153) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4134:0\n",
      "  %155 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={8}]()\n",
      "  %156 : LongTensor = onnx::Div(%154, %155)\n",
      "  %157 : Tensor = onnx::Cast[to=7](%156)\n",
      "  %158 : Long(requires_grad=0, device=cpu) = onnx::Cast[to=7](%157) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\tensor.py:551:0\n",
      "  %162 : Tensor = onnx::Unsqueeze[axes=[0]](%154)\n",
      "  %164 : Tensor = onnx::Constant[value={1}]()\n",
      "  %165 : Float(128:128, 128:1, requires_grad=1, device=cpu) = onnx::Slice(%main_model.transformer.encoder.self_attn.in_proj_weight, %391, %162, %392, %164) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4179:0\n",
      "  %169 : Tensor = onnx::Unsqueeze[axes=[0]](%154)\n",
      "  %171 : Tensor = onnx::Constant[value={1}]()\n",
      "  %172 : Float(128:1, requires_grad=1, device=cpu) = onnx::Slice(%main_model.transformer.encoder.self_attn.in_proj_bias, %393, %169, %394, %171) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4181:0\n",
      "  %173 : Float(128:1, 128:128, requires_grad=1, device=cpu) = onnx::Transpose[perm=[1, 0]](%165) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1692:0\n",
      "  %174 : Float(400:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::MatMul(%144, %173) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1692:0\n",
      "  %175 : Float(400:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::Add(%174, %172)\n",
      "  %176 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={2}]()\n",
      "  %177 : Long(requires_grad=0, device=cpu) = onnx::Mul(%154, %176)\n",
      "  %179 : Tensor = onnx::Unsqueeze[axes=[0]](%154)\n",
      "  %180 : Tensor = onnx::Unsqueeze[axes=[0]](%177)\n",
      "  %182 : Tensor = onnx::Constant[value={1}]()\n",
      "  %183 : Float(128:128, 128:1, requires_grad=1, device=cpu) = onnx::Slice(%main_model.transformer.encoder.self_attn.in_proj_weight, %179, %180, %395, %182) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4188:0\n",
      "  %185 : Tensor = onnx::Unsqueeze[axes=[0]](%154)\n",
      "  %186 : Tensor = onnx::Unsqueeze[axes=[0]](%177)\n",
      "  %188 : Tensor = onnx::Constant[value={1}]()\n",
      "  %189 : Float(128:1, requires_grad=1, device=cpu) = onnx::Slice(%main_model.transformer.encoder.self_attn.in_proj_bias, %185, %186, %396, %188) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4190:0\n",
      "  %190 : Float(128:1, 128:128, requires_grad=1, device=cpu) = onnx::Transpose[perm=[1, 0]](%183) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1692:0\n",
      "  %191 : Float(464:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::MatMul(%145, %190) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1692:0\n",
      "  %192 : Float(464:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::Add(%191, %189) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4265:0\n",
      "  %193 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={2}]()\n",
      "  %194 : Long(requires_grad=0, device=cpu) = onnx::Mul(%154, %193)\n",
      "  %197 : Tensor = onnx::Unsqueeze[axes=[0]](%194)\n",
      "  %200 : Tensor = onnx::Constant[value={1}]()\n",
      "  %201 : Float(128:128, 128:1, requires_grad=1, device=cpu) = onnx::Slice(%main_model.transformer.encoder.self_attn.in_proj_weight, %197, %397, %398, %200) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4197:0\n",
      "  %204 : Tensor = onnx::Unsqueeze[axes=[0]](%194)\n",
      "  %207 : Tensor = onnx::Constant[value={1}]()\n",
      "  %208 : Float(128:1, requires_grad=1, device=cpu) = onnx::Slice(%main_model.transformer.encoder.self_attn.in_proj_bias, %204, %399, %400, %207) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4199:0\n",
      "  %209 : Float(128:1, 128:128, requires_grad=1, device=cpu) = onnx::Transpose[perm=[1, 0]](%201) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1692:0\n",
      "  %210 : Float(464:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::MatMul(%141, %209) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1692:0\n",
      "  %211 : Float(464:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::Add(%210, %208) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4267:0\n",
      "  %212 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={0.25}]()\n",
      "  %213 : Float(400:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::Mul(%175, %212) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4263:0\n",
      "  %214 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={8}]()\n",
      "  %215 : Long(requires_grad=0, device=cpu) = onnx::Mul(%151, %214)\n",
      "  %216 : Tensor = onnx::Unsqueeze[axes=[0]](%148)\n",
      "  %217 : Tensor = onnx::Unsqueeze[axes=[0]](%215)\n",
      "  %218 : Tensor = onnx::Unsqueeze[axes=[0]](%158)\n",
      "  %219 : Tensor = onnx::Concat[axis=0](%216, %217, %218)\n",
      "  %220 : Float(400:128, 8:16, 16:1, requires_grad=1, device=cpu) = onnx::Reshape(%213, %219) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4263:0\n",
      "  %221 : Float(8:16, 400:128, 16:1, requires_grad=1, device=cpu) = onnx::Transpose[perm=[1, 0, 2]](%220) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4263:0\n",
      "  %222 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={8}]()\n",
      "  %223 : Long(requires_grad=0, device=cpu) = onnx::Mul(%151, %222)\n",
      "  %226 : Tensor = onnx::Unsqueeze[axes=[0]](%223)\n",
      "  %227 : Tensor = onnx::Unsqueeze[axes=[0]](%158)\n",
      "  %228 : Tensor = onnx::Concat[axis=0](%401, %226, %227)\n",
      "  %229 : Float(464:128, 8:16, 16:1, requires_grad=1, device=cpu) = onnx::Reshape(%192, %228) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4265:0\n",
      "  %230 : Float(8:16, 464:128, 16:1, requires_grad=1, device=cpu) = onnx::Transpose[perm=[1, 0, 2]](%229) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4265:0\n",
      "  %231 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={8}]()\n",
      "  %232 : Long(requires_grad=0, device=cpu) = onnx::Mul(%151, %231)\n",
      "  %235 : Tensor = onnx::Unsqueeze[axes=[0]](%232)\n",
      "  %236 : Tensor = onnx::Unsqueeze[axes=[0]](%158)\n",
      "  %237 : Tensor = onnx::Concat[axis=0](%402, %235, %236)\n",
      "  %238 : Float(464:128, 8:16, 16:1, requires_grad=1, device=cpu) = onnx::Reshape(%211, %237) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4267:0\n",
      "  %239 : Float(8:16, 464:128, 16:1, requires_grad=1, device=cpu) = onnx::Transpose[perm=[1, 0, 2]](%238) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4267:0\n",
      "  %240 : Tensor = onnx::Shape(%230)\n",
      "  %241 : Tensor = onnx::Constant[value={1}]()\n",
      "  %242 : Long(device=cpu) = onnx::Gather[axis=0](%240, %241) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4279:0\n",
      "  %243 : Float(8:16, 16:1, 464:128, requires_grad=1, device=cpu) = onnx::Transpose[perm=[1, 2, 0]](%229) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4294:0\n",
      "  %244 : Float(8:185600, 400:464, 464:1, requires_grad=1, device=cpu) = onnx::MatMul(%221, %243) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4294:0\n",
      "  %246 : Tensor = onnx::Unsqueeze[axes=[0]](%151)\n",
      "  %248 : Tensor = onnx::Unsqueeze[axes=[0]](%148)\n",
      "  %249 : Tensor = onnx::Unsqueeze[axes=[0]](%242)\n",
      "  %250 : Tensor = onnx::Concat[axis=0](%246, %403, %248, %249)\n",
      "  %251 : Float(1:1484800, 8:185600, 400:464, 464:1, requires_grad=1, device=cpu) = onnx::Reshape(%244, %250) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4305:0\n",
      "  %252 : Bool(1:464, 1:464, 464:1, requires_grad=0, device=cpu) = onnx::Unsqueeze[axes=[1]](%142) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4307:0\n",
      "  %253 : Bool(1:464, 1:464, 1:464, 464:1, requires_grad=0, device=cpu) = onnx::Unsqueeze[axes=[2]](%252) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4307:0\n",
      "  %254 : Tensor = onnx::Cast[to=9](%253)\n",
      "  %255 : Tensor = onnx::Constant[value={-inf}]()\n",
      "  %256 : Float(1:1484800, 8:185600, 400:464, 464:1, requires_grad=1, device=cpu) = onnx::Where(%254, %255, %251) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4308:0\n",
      "  %257 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={8}]()\n",
      "  %258 : Long(requires_grad=0, device=cpu) = onnx::Mul(%151, %257)\n",
      "  %259 : Tensor = onnx::Unsqueeze[axes=[0]](%258)\n",
      "  %260 : Tensor = onnx::Unsqueeze[axes=[0]](%148)\n",
      "  %261 : Tensor = onnx::Unsqueeze[axes=[0]](%242)\n",
      "  %262 : Tensor = onnx::Concat[axis=0](%259, %260, %261)\n",
      "  %263 : Float(8:185600, 400:464, 464:1, requires_grad=1, device=cpu) = onnx::Reshape(%256, %262) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4310:0\n",
      "  %264 : Float(8:185600, 400:464, 464:1, requires_grad=1, device=cpu) = onnx::Softmax[axis=2](%263) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:983:0\n",
      "  %265 : Float(8:6400, 400:16, 16:1, requires_grad=1, device=cpu) = onnx::MatMul(%264, %239) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4316:0\n",
      "  %266 : Float(400:128, 8:16, 16:1, requires_grad=1, device=cpu) = onnx::Transpose[perm=[1, 0, 2]](%265) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4318:0\n",
      "  %267 : Tensor = onnx::Unsqueeze[axes=[0]](%148)\n",
      "  %268 : Tensor = onnx::Unsqueeze[axes=[0]](%151)\n",
      "  %269 : Tensor = onnx::Unsqueeze[axes=[0]](%154)\n",
      "  %270 : Tensor = onnx::Concat[axis=0](%267, %268, %269)\n",
      "  %271 : Float(400:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::Reshape(%266, %270) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:4318:0\n",
      "  %273 : Float(400:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::MatMul(%271, %404) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1692:0\n",
      "  %274 : Float(400:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::Add(%273, %main_model.transformer.encoder.self_attn.out_proj.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:983:0\n",
      "  %275 : Float(400:1, 1:51200, 128:400, requires_grad=1, device=cpu) = onnx::Add(%144, %274) # ..\\lib\\models\\stark\\lite_encoder.py:38:0\n",
      "  %277 : Tensor = onnx::ReduceMean[axes=[-1]](%275)\n",
      "  %278 : FloatTensor = onnx::Sub(%275, %277)\n",
      "  %279 : Tensor = onnx::Cast[to=1](%278)\n",
      "  %281 : Tensor = onnx::Pow(%279, %405)\n",
      "  %282 : Tensor = onnx::ReduceMean[axes=[-1]](%281)\n",
      "  %283 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={1e-05}]()\n",
      "  %284 : FloatTensor = onnx::Add(%282, %283)\n",
      "  %285 : Tensor = onnx::Sqrt(%284)\n",
      "  %286 : FloatTensor = onnx::Div(%278, %285)\n",
      "  %287 : FloatTensor = onnx::Mul(%286, %main_model.transformer.encoder.norm1.weight)\n",
      "  %288 : Float(400:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::Add(%287, %main_model.transformer.encoder.norm1.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:2095:0\n",
      "  %290 : Float(400:1024, 1:1024, 1024:1, requires_grad=1, device=cpu) = onnx::MatMul(%288, %406) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1692:0\n",
      "  %291 : Float(400:1024, 1:1024, 1024:1, requires_grad=1, device=cpu) = onnx::Add(%290, %main_model.transformer.encoder.linear1.bias)\n",
      "  %292 : Float(400:1024, 1:1024, 1024:1, requires_grad=1, device=cpu) = onnx::Relu(%291) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:983:0\n",
      "  %294 : Float(400:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::MatMul(%292, %407) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1692:0\n",
      "  %295 : Float(400:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::Add(%294, %main_model.transformer.encoder.linear2.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:983:0\n",
      "  %296 : Float(400:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::Add(%288, %295) # ..\\lib\\models\\stark\\lite_encoder.py:42:0\n",
      "  %298 : Tensor = onnx::ReduceMean[axes=[-1]](%296)\n",
      "  %299 : FloatTensor = onnx::Sub(%296, %298)\n",
      "  %300 : Tensor = onnx::Cast[to=1](%299)\n",
      "  %302 : Tensor = onnx::Pow(%300, %408)\n",
      "  %303 : Tensor = onnx::ReduceMean[axes=[-1]](%302)\n",
      "  %304 : Float(requires_grad=0, device=cpu) = onnx::Constant[value={1e-05}]()\n",
      "  %305 : FloatTensor = onnx::Add(%303, %304)\n",
      "  %306 : Tensor = onnx::Sqrt(%305)\n",
      "  %307 : FloatTensor = onnx::Div(%299, %306)\n",
      "  %308 : FloatTensor = onnx::Mul(%307, %main_model.transformer.encoder.norm2.weight)\n",
      "  %309 : Float(400:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::Add(%308, %main_model.transformer.encoder.norm2.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:2095:0\n",
      "  %310 : Tensor = onnx::Constant[value={0}]()\n",
      "  %311 : Tensor = onnx::Constant[value={-400}]()\n",
      "  %312 : Tensor = onnx::Constant[value={9223372036854775807}]()\n",
      "  %313 : Tensor = onnx::Constant[value={1}]()\n",
      "  %314 : Float(400:128, 1:128, 128:1, requires_grad=1, device=cpu) = onnx::Slice(%309, %311, %312, %310, %313) # ..\\tracking\\ORT_lightning_X_trt_complete.py:70:0\n",
      "  %315 : Float(1:51200, 128:400, 400:1, requires_grad=1, device=cpu) = onnx::Transpose[perm=[1, 2, 0]](%314) # ..\\tracking\\ORT_lightning_X_trt_complete.py:70:0\n",
      "  %316 : Tensor = onnx::Shape(%315)\n",
      "  %317 : Tensor = onnx::Constant[value={0}]()\n",
      "  %318 : Long(device=cpu) = onnx::Gather[axis=0](%316, %317) # ..\\tracking\\ORT_lightning_X_trt_complete.py:71:0\n",
      "  %319 : Tensor = onnx::Shape(%315)\n",
      "  %320 : Tensor = onnx::Constant[value={1}]()\n",
      "  %321 : Long(device=cpu) = onnx::Gather[axis=0](%319, %320) # ..\\tracking\\ORT_lightning_X_trt_complete.py:71:0\n",
      "  %324 : Tensor = onnx::Unsqueeze[axes=[0]](%318)\n",
      "  %325 : Tensor = onnx::Unsqueeze[axes=[0]](%321)\n",
      "  %328 : Tensor = onnx::Concat[axis=0](%324, %325, %409, %410)\n",
      "  %329 : Float(1:51200, 128:400, 20:20, 20:1, requires_grad=1, device=cpu) = onnx::Reshape(%315, %328) # ..\\tracking\\ORT_lightning_X_trt_complete.py:71:0\n",
      "  %330 : Float(1:51200, 128:400, 20:20, 20:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%329, %main_model.box_head.conv_tower.0.rbr_reparam.weight, %main_model.box_head.conv_tower.0.rbr_reparam.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\conv.py:420:0\n",
      "  %331 : Float(1:51200, 128:400, 20:20, 20:1, requires_grad=1, device=cpu) = onnx::Relu(%330) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1136:0\n",
      "  %332 : Float(1:51200, 128:400, 20:20, 20:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%331, %main_model.box_head.conv_tower.1.rbr_reparam.weight, %main_model.box_head.conv_tower.1.rbr_reparam.bias) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\modules\\conv.py:420:0\n",
      "  %333 : Float(1:51200, 128:400, 20:20, 20:1, requires_grad=1, device=cpu) = onnx::Relu(%332) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1136:0\n",
      "  %334 : Float(1:800, 2:400, 20:20, 20:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[1, 1, 1, 1], strides=[1, 1]](%333, %main_model.box_head.conv_tower.2.weight, %main_model.box_head.conv_tower.2.bias) # ..\\lib\\models\\stark\\head.py:257:0\n",
      "  %335 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={0}]()\n",
      "  %336 : Float(1:800, 20:20, 20:1, requires_grad=1, device=cpu) = onnx::Gather[axis=1](%334, %335) # ..\\lib\\models\\stark\\head.py:257:0\n",
      "  %337 : Long(requires_grad=0, device=cpu) = onnx::Constant[value={1}]()\n",
      "  %338 : Float(1:800, 20:20, 20:1, requires_grad=1, device=cpu) = onnx::Gather[axis=1](%334, %337) # ..\\lib\\models\\stark\\head.py:257:0\n",
      "  %339 : Tensor = onnx::Constant[value=  -1  400 [ CPULongType{2} ]]()\n",
      "  %340 : Float(1:400, 400:1, requires_grad=1, device=cpu) = onnx::Reshape(%336, %339) # ..\\lib\\models\\stark\\head.py:261:0\n",
      "  %341 : Float(1:400, 400:1, requires_grad=1, device=cpu) = onnx::Softmax[axis=1](%340) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1512:0\n",
      "  %342 : Float(400:1, requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>]()\n",
      "  %343 : Float(1:400, 400:1, requires_grad=1, device=cpu) = onnx::Mul(%342, %341) # ..\\lib\\models\\stark\\head.py:263:0\n",
      "  %344 : Float(1:1, requires_grad=1, device=cpu) = onnx::ReduceSum[axes=[1], keepdims=0](%343) # ..\\lib\\models\\stark\\head.py:263:0\n",
      "  %345 : Float(400:1, requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>]()\n",
      "  %346 : Float(1:400, 400:1, requires_grad=1, device=cpu) = onnx::Mul(%345, %341) # ..\\lib\\models\\stark\\head.py:264:0\n",
      "  %347 : Float(1:1, requires_grad=1, device=cpu) = onnx::ReduceSum[axes=[1], keepdims=0](%346) # ..\\lib\\models\\stark\\head.py:264:0\n",
      "  %348 : Tensor = onnx::Constant[value=  -1  400 [ CPULongType{2} ]]()\n",
      "  %349 : Float(1:400, 400:1, requires_grad=1, device=cpu) = onnx::Reshape(%338, %348) # ..\\lib\\models\\stark\\head.py:261:0\n",
      "  %350 : Float(1:400, 400:1, requires_grad=1, device=cpu) = onnx::Softmax[axis=1](%349) # C:\\Users\\isd.illia.maliha\\Anaconda3\\envs\\py36\\lib\\site-packages\\torch\\nn\\functional.py:1512:0\n",
      "  %351 : Float(400:1, requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>]()\n",
      "  %352 : Float(1:400, 400:1, requires_grad=1, device=cpu) = onnx::Mul(%351, %350) # ..\\lib\\models\\stark\\head.py:263:0\n",
      "  %353 : Float(1:1, requires_grad=1, device=cpu) = onnx::ReduceSum[axes=[1], keepdims=0](%352) # ..\\lib\\models\\stark\\head.py:263:0\n",
      "  %354 : Float(400:1, requires_grad=0, device=cpu) = onnx::Constant[value=<Tensor>]()\n",
      "  %355 : Float(1:400, 400:1, requires_grad=1, device=cpu) = onnx::Mul(%354, %350) # ..\\lib\\models\\stark\\head.py:264:0\n",
      "  %356 : Float(1:1, requires_grad=1, device=cpu) = onnx::ReduceSum[axes=[1], keepdims=0](%355) # ..\\lib\\models\\stark\\head.py:264:0\n",
      "  %357 : Tensor = onnx::Unsqueeze[axes=[1]](%344)\n",
      "  %358 : Tensor = onnx::Unsqueeze[axes=[1]](%347)\n",
      "  %359 : Tensor = onnx::Unsqueeze[axes=[1]](%353)\n",
      "  %360 : Tensor = onnx::Unsqueeze[axes=[1]](%356)\n",
      "  %361 : Float(1:4, 4:1, requires_grad=1, device=cpu) = onnx::Concat[axis=1](%357, %358, %359, %360) # ..\\lib\\models\\stark\\head.py:253:0\n",
      "  %364 : Float(1:4, 4:1, requires_grad=1, device=cpu) = onnx::Div(%361, %411)\n",
      "  %365 : Tensor, %366 : Tensor, %367 : Tensor, %368 : Tensor = onnx::Split[axis=-1, split=[1, 1, 1, 1]](%364)\n",
      "  %369 : Float(1:4, requires_grad=1, device=cpu) = onnx::Squeeze[axes=[-1]](%365) # ..\\lib\\utils\\box_ops.py:26:0\n",
      "  %370 : Float(1:4, requires_grad=1, device=cpu) = onnx::Squeeze[axes=[-1]](%366) # ..\\lib\\utils\\box_ops.py:26:0\n",
      "  %371 : Float(1:4, requires_grad=1, device=cpu) = onnx::Squeeze[axes=[-1]](%367) # ..\\lib\\utils\\box_ops.py:26:0\n",
      "  %372 : Float(1:4, requires_grad=1, device=cpu) = onnx::Squeeze[axes=[-1]](%368) # ..\\lib\\utils\\box_ops.py:26:0\n",
      "  %373 : Float(1:1, requires_grad=1, device=cpu) = onnx::Add(%369, %371) # ..\\lib\\utils\\box_ops.py:27:0\n",
      "  %376 : Float(1:1, requires_grad=1, device=cpu) = onnx::Div(%373, %412)\n",
      "  %377 : Float(1:1, requires_grad=1, device=cpu) = onnx::Add(%370, %372) # ..\\lib\\utils\\box_ops.py:27:0\n",
      "  %380 : Float(1:1, requires_grad=1, device=cpu) = onnx::Div(%377, %413)\n",
      "  %381 : Float(1:1, requires_grad=1, device=cpu) = onnx::Sub(%371, %369) # ..\\lib\\utils\\box_ops.py:28:0\n",
      "  %382 : Float(1:1, requires_grad=1, device=cpu) = onnx::Sub(%372, %370) # ..\\lib\\utils\\box_ops.py:28:0\n",
      "  %383 : Tensor = onnx::Unsqueeze[axes=[-1]](%376)\n",
      "  %384 : Tensor = onnx::Unsqueeze[axes=[-1]](%380)\n",
      "  %385 : Tensor = onnx::Unsqueeze[axes=[-1]](%381)\n",
      "  %386 : Tensor = onnx::Unsqueeze[axes=[-1]](%382)\n",
      "  %outputs_coord : Float(1:4, 4:1, requires_grad=1, device=cpu) = onnx::Concat[axis=-1](%383, %384, %385, %386) # ..\\lib\\utils\\box_ops.py:29:0\n",
      "  return (%outputs_coord)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(model, (img_x, mask_x.type(torch.FloatTensor), feat_vec_z, mask_vec_z.type(torch.FloatTensor), pos_vec_z),\n",
    "                  model_one, \n",
    "                  input_names=['img_x', 'mask_x', 'feat_vec_z', 'mask_vec_z', 'pos_vec_z'],\n",
    "                  output_names=['outputs_coord'],\n",
    "                  verbose=True, opset_version=11, export_params=True, \n",
    "                  do_constant_folding=True) \n",
    "\n",
    "# simplified_model, check = simplify(model_one, skip_fuse_bn=False)\n",
    "# onnx.save_model(simplified_model, model_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c522d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx_model = onnxruntime.InferenceSession(model_one, providers=['CPUExecutionProvider'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d185626",
   "metadata": {},
   "source": [
    "###### Quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ad0bd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quant = \"../networks/onnx/complete_one_int8_static.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16e303e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_image_folder = \"C:\\\\Users\\\\isd.illia.maliha\\\\work\\\\sorted_datasets\\\\background\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a10c29db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_images(backbone_path: str, images_folder: str, size_limit=0):\n",
    "    bone = onnxruntime.InferenceSession(backbone_path, providers=['CPUExecutionProvider'])\n",
    "    inp_names = [bone.get_inputs()[i].name for i in range(2)]\n",
    "    \n",
    "    image_names = os.listdir(images_folder)\n",
    "    \n",
    "    if size_limit > 0 and len(image_names) >= size_limit:\n",
    "        batch_filenames = [image_names[i] for i in range(size_limit)]\n",
    "    else:\n",
    "        batch_filenames = image_names\n",
    "    in1,in2,in3,in4,in5 = [],[],[],[],[]\n",
    " \n",
    "    preprocessor = PreprocessorX_onnx()\n",
    "\n",
    "    for image_name in tqdm_notebook(batch_filenames[:]):\n",
    "        image_filepath = images_folder + \"/\" + image_name\n",
    "        image = cv2.cvtColor(cv2.imread(image_filepath), cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        \n",
    "        h,w,_ = image.shape\n",
    "        x = np.random.randint(int(0.4*w), int(0.6*w))\n",
    "        y = np.random.randint(int(0.4*h), int(0.6*h))\n",
    "        ww = int(0.1*w)\n",
    "        hh = int(0.1*h)\n",
    "        \n",
    "        z_patch_arr, _, z_amask_arr = sample_target(image.copy(), [x,y,ww,hh], 2.0, 128)\n",
    "        template, template_mask = preprocessor.process(z_patch_arr, z_amask_arr)\n",
    "        \n",
    "        inp_dict = {inp_names[0]: template,\n",
    "                    inp_names[1]: template_mask}\n",
    "        bone_out = bone.run(None, inp_dict)\n",
    "    \n",
    "        x_patch_arr, _, x_amask_arr = sample_target(image.copy(), [x,y,ww,hh], 5.0, output_sz=320)\n",
    "        template, template_mask = preprocessor.process(x_patch_arr, x_amask_arr)\n",
    "        \n",
    "        img_x = torch.Tensor(template)\n",
    "        mask_x = torch.Tensor(template_mask) \n",
    "        feat_vec_z = torch.Tensor(bone_out[0])\n",
    "        mask_vec_z = torch.Tensor(bone_out[1])\n",
    "        pos_vec_z = torch.Tensor(bone_out[2])\n",
    "\n",
    "        mask_x = mask_x[:,None]\n",
    "\n",
    "        feat_vec_z = feat_vec_z.permute((1,0,2))[:,None]\n",
    "        pos_vec_z  = pos_vec_z.permute((1,0,2))[:,None]\n",
    "        fp_z = torch.cat([feat_vec_z, pos_vec_z], dim=2)\n",
    "        fp_z = F.pad(fp_z,(96,96,96,96))\n",
    "\n",
    "        mask_vec_z = mask_vec_z[:, None, None]\n",
    "        mask_vec_z = F.pad(mask_vec_z,(128,128,159,160))\n",
    "\n",
    "        inp = torch.cat([img_x, mask_x, fp_z, mask_vec_z], dim=1)\n",
    "        inp = inp.reshape((1,3,320,640))\n",
    "\n",
    "        in1.append(inp.numpy())\n",
    "\n",
    "\n",
    "    batch_data1 = np.concatenate(np.expand_dims(in1, axis=0), axis=0)\n",
    "    \n",
    "#     print(batch_data1.shape)\n",
    "    \n",
    "    return batch_data1\n",
    "\n",
    "\n",
    "class DataReader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_image_folder: str, model_path: str, backbone_path: str):\n",
    "        self.enum_data = None\n",
    "\n",
    "        # Use inference session to get input shape.\n",
    "        session = onnxruntime.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
    "\n",
    "        # Convert image to input data\n",
    "        self.nchw_data_list = _preprocess_images(backbone_path, calibration_image_folder, size_limit=0)\n",
    "        \n",
    "        self.input_name = session.get_inputs()[0].name\n",
    "        self.datasize = len(self.nchw_data_list)\n",
    "    \n",
    "\n",
    "    def get_next(self):\n",
    "        if self.enum_data is None:\n",
    "            self.enum_data = iter(\n",
    "                [{self.input_name: self.nchw_data_list[idx]} for idx in range(self.datasize)]\n",
    "            )\n",
    "        return next(self.enum_data, None)\n",
    "\n",
    "    def rewind(self):\n",
    "        self.enum_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78b8004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m onnxruntime.quantization.preprocess --input \"../networks/onnx/complete_one.onnx\" --output \"../networks/onnx/complete_one.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c3c2e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5688d03ef54032894777242208ca09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=882), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reader = DataReader(calibration_image_folder, model_one, backbone_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f14f0fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = quantize_static(model_one,\n",
    "                                  model_quant,\n",
    "                                  reader,\n",
    "                                  quant_format=QuantFormat.QDQ,\n",
    "                                  activation_type=QuantType.QInt8,\n",
    "                                  weight_type=QuantType.QInt8,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cbfec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_model, check = simplify(model_quant, skip_fuse_bn=False)\n",
    "onnx.save_model(simplified_model, model_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53fdf268",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = onnxruntime.InferenceSession(model_quant, providers=['CPUExecutionProvider'])\n",
    "\n",
    "for x in session.get_outputs():\n",
    "    print(x.name)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
